 Hashing and Hash Tables


Introduction:

Hash tables are a data structure for storing and retrieving unordered
information, whose primary operations are in complexity class O(1) -
independent of the number of items stored in the hash table. We saw that
digital trees had this same property, but only for special keys (that were
digital: meaning we could decompose them into a first part of the key, a second
part of the key, etc. as we can with digits in a number and characters in a
String). Hash tables work with any kind of key. The most commonly used
implementations of sets and maps (which are unordered) are implemented by hash
tables. We will also implement a map via a hash table (including iterators) in
Program #4.

Here are some terms that we need to become familiar with to understand (and
talk about) hash tables: hash codes, compression function, bins/buckets, 
overflow-chaining, probing, load factor, and open-addressing. We will discuss
each below.


Our Approach:

We will start by discussing linear searching (using a linked list) of a
collection of names. If we instead used an array of 26 indexes and put in
index 0 a linked list of all names starting with "a", and in index 1 a linked
list of all names starting with "b", ... and in index 25 a linked list of all
names starting with "z", we could search for a name about 26 times faster by
looking just in the correct index for any name (according to its first letter).
This speed increase assumes each letter is equally likely to start a last name,
which is not a realistic assumption. Even if the assumption is true, the
complexity class for searching is still O(N), but with a constant 1/26th less
than the original constant. Meaning, however long it takes to search among N
names, it would take 1,000 times as long to search among 1,000 names: each of
the linked lists's length would grow by a factor of 1,000.

In fact, if we used an array of 26x26 (676) indexes, storing in index 0 a
linked list of all names starting with "aa", and in index 1 a linked list of
all names starting with "ab", ... and in index 676 a linked list of all names
starting with "zz", we could search for a name 676 times faster by looking
just in the correct index for any name (according to its first two letters).
This speed increase assumes each letter pair is equally likely to start a last
name, which is an even less realistic assumption. Even if the assumption is
true, the complexity class for searching is still O(N), but with a constant of
1/676th less than the original constant.

Of course, this speedup isn't achieved unless we have many names, and each box
is equally likely to have a name (which isn't true: few names start with
combinations like "bb", etc). And what about looking-up information that
aren't strings: for example, we might want a set storing queues or a map whose
keys are priority queues. So while this approach seems promising, we need to
modify it to be truly useful.


------------------------------------------------------------------------------

Hash Codes:

Hashing is that modification. We declare an array with any number of "bins or
buckets" and use a "hash code function" to compute an integer index for any
piece of data that can go into the hash table - indicating in which bin/bucket
it belongs. A hash code function must always compute the same hash code for the
same value, so it cannot use random numbers. We should design such a hash code
function to generate the widest variety of numbers (over the range of all
integers), with as small a probability as possible of two different values
hashing to the same integer.

Of course, in the case of using strings as values, there are more strings than
integer values. For 32-bit ints, there are only about 4 billion different
values -actually, exactly 4,294,967,296- but an infinite number of strings,
which can be of any length, meaning any number of characters: even if we
consider only strings with lower-case letters, there are 26^N different strings
with N chacters; 26^7 is 8,031,810,176 so there are already more 7-letter
strings than 32-bit ints.

Once we have a hash code function, we use a "compression function" to convert
the hash code into a legal index in our hash table. One simple compression
function computes the absolute value of the hash code (hash codes should cover
both negative and positive values but array indexes are always non-negative)
and then computes the remainder (using the % operator) using the hash table
size/length as the 2nd operand, producing a number between 0 and length-1 of the
hash table. Other compression functions use-bitwise operations to compute a bit
pattern in the correct range (doing so is faster than computing a remainder,
but it typically works only for hash table sizes/lengths that are a perfect
power of 2: of course we can arrange hash table to start at length/size 1 and
always double their size to meet this constraint).

Here is one example of a hash code function for strings.

  int hash(std:string s) {
    int hash_code = 0;
    for (int i = 0; i  int: its ASCII value
    return hash_code;
  }   

Here, hash("a") returns 97 ('a' has an ASCII value of 97) and hash("aa")
returns 3104 (31*97 + 97). Generally, if s.size() is n (the chars array
contains n values), then its hash value is given by the formula

   chars[0]*31^(n-1) + chars[1]*31^(n-2) + ... + chars[n-2]*31^1 + chars[n-1]

So, hash("ICS46") returns 69,494,459, and hash("Richard Pattis")
returns -125,886,044! Yes, because of arithmetic overflow and the standard
properties of binary numbers, the result might be negative (and overflow of
negative numbers can go positive again). Recall that C++ does not throw any
exceptions when arithmetic operators produce values outside of the range of
int: hashing is one of the few places where this behavior produces results
that are still useful. 

Generally the hash function for all the numeric types is an int value with
the same bit pattern (just interpret those bits as an int). Characters hash to
their ASCII values. We can build hash functions for other types in C++ from
from these: for examples strings, are an ORDERED sequence of chars and we can
compute the hash code of the string by looking at every char in it. Note
that generally hash("ab") != hash("ba"). When hashing strings, the order of the
letters is important. In fact, a good hash code function for any ordered data
type (say a queue or a stack) will produce different values for different
orders of the same values.

In C++ we can use the following to compute the hash codes. Here
std::hash is a class whose default constructor initializes
str_hash, which overloads call -the () operator- to compute a hash value for
std::string.

  std::hash str_hash;
  std::cout  int
    return cachedHash = hash;
  }   

If a String's computed hashCode is 0, even after its hashCode is computed and
cached, it will be recomputed (because with the != 0 test, Java cannot tell
the difference between a hash code that has not been computed and a hash code
that has been computed with value 0). Typically, the only String whose hash
code is 0 is ""; most other Strings will have a non-0 hashCode. Recomputing the
hash code of "" is very quick, because it stores no values (chars.length is 0,
so the loop immediately exits). We could include an extra boolean instance
variable named hashCached, initialized to false and set it to true after
caching. So we would have

  public int hashCode() {
    if (hashCached)
      return cachedHash;

    int hash = 0;
    for (int i = 0; i  int

    hashCached = true;
    return cachedHash = hash;
  }   

...but that extra instance variables is a bit overkill. 

Of course, we also could compute the hashCode of every String WHEN IT IS
CONSTRUCTED, storing it in cachedHash, and never checking this value, always
returning the cached value. The upside is that the hashCode function would
always just return this cached value; the downside is that we would have to
compute the hash code for every String when it was created, even if we were
never are going to call hashCode on it. The approach above, actually used in
Java, caches a hash code only if asked to compute it at least once.

------------------------------------------------------------------------------

Hash Tables with Overflow-Chaining

Hash Tables are used frequently in practice, so there has been a lot of
studies, both theoretical and empirical, of hash code methods, which are
at the heart of Hash Tables working efficiently. The best methods are quick
to compute and return results scattered all over the range of int. Given such
a hash code, the rest of the code to implement a Hash Table (see below) is
straightforward.

Accompanying this lecture is a program and an example of the results produced
by running the program, which empirically examines the hash code function shown
early in this lecture for strings (and you can write your own hash code to test
too). The program itself was originally written in Java and I converted it to
C++. It allows students to posit various hash code functions and test them
by generating random strings (which is an easy way to test a hash function, but
may not accruately model the strings used in a real application), putting them
in the hash table, and counting the collisions: when two different values
Hash/Compress to the same bin. We will look at this data briefly.

Now let's look at how to insert a value into a hash table. We will assume that
as in sets and map keys, the values are distinct. Recall the basic structure
used in a hash table is an array of bins/buckets with each referring to a
linked list of values that hash/compress to that index. The basic picture looks
like the following (here with 5 bins/buckets, 0-4).

   Bin/Bucket     Collisions (handled through chaining, using a linked list)

     +---+
     |   |    +----+---+    +----+---+
  0  | ------&gt;| v1 | --+---&gt;| v2 | / |
     |   |    +--------+    +--------+
     +---+
     |   |
  1  | / |
     |   |
     +---+
     |   |    +----+---+
  2  | ------&gt;| v3 | / |
     |   |    +--------+
     +---+
     |   |    +----+---+    +----+---+
  3  | ------&gt;| v4 | --+---&gt;| v5 | / |
     |   |    +--------+    +--------+
     +---+
     |   |    +----+---+
  4  | ------&gt;| v6 | / |
     |   |    +--------+
     +---+

Generally, bins can have zero, one, or many values. We say that values v1 and
v2 COLLIDED in bin 0, because these two different values both hashed/compressed
to the same bin in the hash table. And we have used "overflow chaining" (using a
linked list) to keep track of all the "collisions"/"overflows. With good hash
and compression functions, the values stored in a hash tabled should be
approximately equally distributed throughout the bins. Of course, there will
typically be some bins with fewer values (many may store none) and some bins
with more because hash codes aren't perfect.

------------------------------------------------------------------------------

Hash Table Algorithms:

3 Important Algorithms manipulating Hash Tables

1) insert(for Set)/put or setting with [] and = (for Map):
   Use hash_code/compression to compute a bin index (for the value/key).
   Search all the collisions to see if the information is there
      (sets have unique values/maps have unique keys).
    If it is there: for a Sets don't change anything; for Maps change the value
       associated with the key in the ics::pair.
    If it is not there: add the information anywhere convenient in that
      bin: in a list node at the front, rear, wherever: there is no required
      ordering of information in the bins for sets and maps.

2) contains(for Set)/has_key or lookup with [] (for Map):
   Use hash_code/compression to compute a bin index.
   Search through the linked list (all the collisions) to see if the
     information is there.

3) erase(for Set/Map)
   Use hash_code/compression to compute a bin index.
   Search through the linked list (all the collisions) to see if the
     information is there, and if it is, remove it from the linked list

------------------------------

2 more Important Algorithms

The "Load Factor" of a hash table is the ratio of values it contains divided
by the number of bins in the hash table. It computes the expected number of the
values that will hash/compress to each bin. Generally classes using hash tables
try to keep the load factor below 1 (more bins than values, so each bin
contains zero or very few values -hopefully just 1, but this is typically not
achieved: see empirical analysis).

When the insert/put method is called for a Set/Map, if a new value is to be put
in the hash table, the load factor is checked; if adding the new value will make
the hash table exceed the load factor, we increase the size/length of the hash
table to ensure that it is always below the specified threshold. Typically we
double the size of the hash table, when we must increase its size.

4) Double Size/Length of Hash table
   Remember the old hash table array and allocate a new/empty one 2 times as big
   Traverse the old hash table (you can do it directly or use the Iterator
     if one is available, but that will be slower), adding each value to the
     new hash table, but NOT NECESSARILY IN THE SAME BIN! Instead, add it by
     applying the same hashing/compression again (compression will be DIFFERENT,
     because the size of the new hash table is doubled, so we may compute the
     same hash value, but compute the remainder using the DIFFERENT TABLE SIZE).
   By being clever, we can re-use the entire LN (list node), so we don't have
     to allocate any new objects when doubling the hash table size; but this
     makes the code harder to write.

5) Iterator: Uses a combination of index and cursor instance variables.
   (a) Constructor:
     Loop to find the first bin with a list of values (not nullptr):
       Succeed: set index to the bin number, cursor to the first LN in the bin
       Fail   : set cursor to nullptr

   (b) ++:
     Advance cursor = cursor.next; if it becomes nullptr, loop to advance the
       index to later bins, stopping at the first non-nullptr bin
         Succeed: set index to bin number, cursor to the first LN in the bin
         Fail   : set cursor to nullptr

   (c) Erase in Iterator
     Store previous cursor (in extra instance variable) to help do removal.
       or
     Store no extra information but use a trailer node in every bin (which
       makes removal easier; we will do this in Programming Assignment #4)

Note that iterators for data types implemented by hash tables return their
values in a "strange" order (based on their hash values and collisions). In
fact, adding one value to a hash table can cause it to exceed its load factor,
and thus it will double the number of bins (doubling the size/length of the hash
table) which causes rehashing. Now, the iterator order might be completely
different. So, we should never assume any special ordering for these iterators,
since data types might be implemented by hash table data structures. If we want
to ensure they are processed in a specific order, we should put the values
produced by iterators into a PriorityQueue and iterate through it.

----------
Note that in the STL there are maps and unordered maps. Regular maps are
typically implemented by BST (or self-balancing trees) whereas unordred maps
are typically implemented by hash tables. Generally hash table operations are
faster than trees: O(1) vs O(Log N). But if we must iterate over the data in
a specific key order frequently, using regular maps might be better; of course
we can always iterate over an unordered map in an ordred way by putting all its
values in a priority queue and then iterate over the priority queue in any
specified (by a function) order, not just the order of the keys in a BST.
---------

Implementing Overflows: Linked Lists and Beyond

Note that we can store in each bin an array of values, a linked list of values,
a binary search tree of values, etc.(in fact, we can use anything that
implements a Set data type). The reason that linked lists, and not more exotic
data structures, are used, is that with a load factor &gt; 1, using more complicated data
structures to handle overflows might be a good idea.

We can even use another Hash Table (with a different hashing function) to
store the overflows.

------------------------------------------------------------------------------

Why are hash tables O(1) with good hash functions:

In the worst case, a hash-code will produce exactly the same value for
everything it hashes (not likely, but possible) so no matter how big the hash
table is, we would go to the same bin and put/search for all N values there.
Such a process would be O(N). But let's assume that we are using a hash_code
that does a pretty good job (as most do: review the empirical data).

For such a good hash code, if the table size/length is M, we would expect to
have to search for N/M values in each bin. Thus, for any given M the method is
O(N/M), but that is just O(N) because M is "a constant" so we remove it from our
big-O notation.

But, we are doing something a bit more subtle. By keeping the load factor = N (say M is always at least N, and sometimes
as much as 2*N, right when the load factor exceeds 1 and we double the
size/length of the hash table). So, M IS NOT A CONSTANT, but it grows linearly
with N. In fact, we know that M &gt;= N. In the "worst case" M = N. That is, since
M &gt;= N

  N/M 2) + 2 (2-&gt;4)
 5- 8    7 = 1 (1-&gt;2) + 2 (2-&gt;4) + 4 (4-&gt;8)
 9-16   15 = 1 (1-&gt;2) + 2 (2-&gt;4) + 4 (4-&gt;8) + 8 (8-&gt;16)
17-32   31 = 1 (1-&gt;2) + 2 (2-&gt;4) + 4 (4-&gt;8) + 8 (8-&gt;16) + 16 (16-&gt;32)
33-64   63 = ....

Notice that for N a perfect power of 2, there are N-1 copies. When N is 1 bigger
than a power of two, there are at most 2*N-3 copies. So, the number of times
that we must copy a piece of data as an array grows linearly and is O(N).
 
Rather than thinking about "adding" sometimes doing very little work and
sometime doing lots, we can think about "adding" doing a bit of extra work (a
constant amount) every time that we call it: really, most of the time we don't
do the work, but every so often we have to do a lot of work, not just for the
new value, but for all the ones before it. This is called amortized complexity.
Still, the total work done for adding N values is just O(N) -it cannot be less
because N*O(1) is O(N). You will study this more in ICS-161.

Another way to think of this is just how much extra work is there to double
the number of values in an array. If the original length is N (say it is a
power of 2), then when we add N more values, we will have to first copy
each of the N values originally in the array into the new array, and then copy
each of the new N values into the array. Thus, overall, adding N more values
into the array requires copying N values and then adding N values without
copying, for a total of 2N operations. The total complexity is still just O(N),
since every add required the actual add and a total of N adds also required
copying a total of N values originally in the array. Think about every add as
counting for itself and one of the copying operations.

------------------------------------------------------------------------------

Mutating a Value in A Hash Table (or BST):

If we are using a BST or hash table to store an object in a Set or a key to a
Map, we should not mutate that object inside the Set/Map. This is because the
place it is stored depends on the value/key: when we put a value into a tree,
we use comparisons to determine in which subtree(s) it belongs in; when we put a
value into a hash table, we compute its hash code to determine which array
index it belongs in.

So, if we mutate a key in a tree or hash table, it probably will not belong
where it currently is: it would be in a different location in a tree or
different bin in a hash table. That is, the code to locate and store a value in
Set/key in a Map is based on using its state (for comparison or hashing). If we
store a value in  a Set/key in a Map, and then mutate it (change its state)
then we may never be able to locate that value/key again.

So, it is a good idea to use immutable classes for keys, or at least be extra
careful not to mutate a value INSIDE a tree/key INSIDE a Map. Instead, we can
remove it, change the key, and then add it back. For a hash table, both removal
and addition are O(1) operations, so although awkward, changing a value in this
way (remove it, change it, add it) only requires a constant amount of work to
update it in the hash table.


------------------------------------------------------------------------------

Hashing with Open Addressing Instead of Chaining:

The final big topic on hashing is collision/overflow resolution without overflow
chaining. By far the most useful way to handle different values that hash to
the same bin is to store all these results in a linear linked list that the
bin points to. Hash table operations will do a linear search of such a list,
which, by having a good hash code and low load factor, will generally not be
long. But this approaches leaves some bins empty (about 1/3 from our empirical
analysis) and linked lists require extra space that must be dynamically created
and deallocated.

There is an alternative way to deal with collisions. While it takes up no extra
space (compared to chaining, which constructs new LN objects) it can cause an
increase in the time needed to search for a value in a hash table, unless the
load factor is kept low ( 1 is not possible: we need a
bin for each value.

The method is called Probing via Open Addressing. We will discuss 3 different
forms of probing: linear, quadratic, and double hashing.

In linear probing, we compute the bin for storing a value; if the table already
contains a value at that bin (we must be able to distinguish indexes with and
without values: often we can use an array of pointers to objects and use
nullptr to detect the absence of data), we increment the index by 1 circularly
(incrementing the last array index brings us back to index 0) and keep probing 
bins until we find an empty bin, and then put the value there.

The find method likewise hashes to a bin and checks of the value is there; if
not, it continues from the original bin, linearly looking through other bins,
until we (a) find the value we are looking for, or (b) reach an empty bin
(meaning the value is not in the hash table; if it were there we would have
reached it before an empty bin). Note that many values can be searched,
including many that have different hash codes that happen to be a bit bigger
than the hash code of the value we are looking for. For example, let's use
linear probing via open addressing for the following hash table.

   0     1     2     3     4    5      6     7     8     9
+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+
|     |     |     |     |     |     |     |     |     |     |
+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+

Thus, if "a" hashes to bin 4, we put it there because it is empty.

   0     1     2     3     4    5      6     7     8     9
+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+
|     |     |     |     | "a" |     |     |     |     |     |
+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+

Likewise, if "b" hashes to bin 5 we put it there because it is empty.

   0     1     2     3     4     5     6     7     8     9
+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+
|     |     |     |     | "a" | "b" |     |     |     |     |
+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+

But now, if "c" hashes to bin 4, we have to probe bin 4 and 5 until we find
that bin 6 is the first empty one after 4, and put "c" there.

   0     1     2     3     4     5     6     7     8     9
+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+
|     |     |     |     | "a" | "b" | "c" |     |     |     |
+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+

If we are looking to see whether "d" is in the table (say it hashes to
bin 4) we start probing at bin 4 for d, then check bin 5, bin 6, and finally
bin 7: it is empty, so we know "d" is not in the hash table (if it were, we
would find it before the empty value). With chaining we would never examine
"b" because it would be in a different bin/bucket from "a", "c", and "d".

Another problem with probing via open addressing involves removing values. If
we want to remove "b", we first try to find it by hashing to bin 5 and find it
there. But if we actually removed it (making it empty), if we were looking
for "c" the following problem would occur: we hash "c" (see above) and get bin
4, then we look at the next bin (which is now empty, because we removed "b")
and we would think that "c" is not in the hash table because we reached an
empty bin first. But at the time "c" was added, there was a value in bin 5,
which is why "c" had to go to bin 6 and not bin 5.

So, in order to know we are done when we reach an empty bin, when we remove a
value, we find it, and mark its bin as "available". Bins marked "available"
had previously stored values and can store new values (if we reach them in the
insertion algorithm), but unlike empty bins they cannot stop the probing:
probing must continue until it reaches the value to locate or has passed
through all occupied and "available" bins and reached a bin that has always
been empty. We can create a special values to represent "empty" and "available".

Iteration over such a structure is easy: it is just a linear traversal of the 
array, skipping unoccupied (empty or available) positions. Likewise we can
easily create a hash table twice as big, iterate through the original hash
table and put (by hashing with a new compression function) each value into the
new hash table.

Instead of linear probling (where the bin number increases by 1 every time),
we can do quadratic probing, where the bin number increases by ai+bi^2 (for
i=0 the first time, i=1 the second time, etc. once we specify the values for
a and b: if both are 1/2, e.g., 1/2(i+i^2) we probe hash, hash+1, hash+3,
hash+6, hash+10, etc). Of course with the compression function handles indexes
that get bigger than the table size/length.

Likewise in double hashing, we use a second hash method h2 (hash, hash+h2(1),
hash+h2(2), hash+h2(3), ... ) to compute a value that is continually added to
the bin index (with the compression function) until the value is located or an
empty bin is found.

Unlike linear probing, by using quadratic probing or double hashing, the probe
sequence "after a bin" depends on how many probes it takes to reach that bin in
the first place. That is, in quadratic probing if we hash to a bin, the next
probe is at hash+1; but if we reach that bin on the third probe (getting there
as hash+6) the next probe is hash+10. This typically improves performance by
spreading out (avoiding clustering) values in the hash table.

Unless space is critical, it is typically better to probe using overflow
chaining than any kind of probing discussed above. Often the extra overhead of
the LN is small compared to its data, although doing chaining requires using
storage allocation/deallocation.

Hash tables using probing via open addressing get clogged up with values in a
non-linear way: as the load factor approaches 1, the searching time approaches
O(N). So, if this method of probing is used, we need to keep the load factor
lower, say at .7. You can simulate such a hash table and measure the
performance degradation by counting the average number of probes at various
load factors.

If you look on the Programs link on the course web site (or the link for this
lecture), you will see a download that allows you to test various hashing
functions statistically. If you want, you can write your own hashing function
and test it compared to the one built-in to C++. There are two drivers there,
one testing chaining and one testing open addressing. It would also be useful
to test hash functions for the amount of time they take to compute their result.


------------------------------------------------------------------------------

Some Mathematics and Hashing: The Birthday Problem

Suppose we had a hash table of size N and a perfectly random hashing function.
How many values would we have to add before the probability of a collision is
50%? The answer might surprise you because it is low. In mathematics, this is
related to "The Birthday Problem": How many people (k people) must be in a room
before there is at least a 50% chance that two have the same birthday?" Here we
assume that each person is equally likely to be born on every day in the year
(which is not true, but is close to true). The answer is nowhere near 365, or
even 180: it is just 23.

Think about the problem this way. We will compute the probability of k people
having DIFFERENT birthdays: then 1 minus that number is the probability of
at least two people sharing a birthday. For everyone to have a different
birthday, the second person must have a birthday different than the first. The
third person must have a birthday different than the first two. The fourth
person must have a birthday different than the first three...

We can compute the probability of k people have different birthdays exactly as

   365-0     365-1      365-2      365-3      365-4             365-(k-1)
  ------- x ------- x  ------- x  ------- x  ------- x  ... x  -----------
    365       365        365        365        365                365

Let us generalize 365 to N, so we can compute the probability of k people having
different birthdays given N=365 days a year, or k values different bin numbers
in a hash table of size N. This product becomes

         N!
P =  -------------
      (N-k)! N**k

If we choose N=365 and we compute this value for different values of k (say in
Excel), we have following data.

 k   |     P
-----+---------------------------
 1   |  100.00%
 2   |   99.73%
 3   |   99.18%
 4   |   98.36%
 5   |   97.29%
 6   |   95.95%
 7   |   94.38%
 8   |   92.57%
 9   |   90.54%
10   |   88.31%
11   |   85.89%
12   |   83.30%
13   |   80.56%
14   |   77.69%
15   |   74.71%
16   |   71.64%
17   |   68.50%
18   |   65.31%
19   |   62.09%
20   |   58.86%
21   |   55.63%
22   |   52.43%
23   |   49.27%  1- &gt;= 50%
24   |   46.17%
25   |   43.13%
26   |   40.18%
...

Thus, if we had a hash table of 365 values, hashing 23 or more values into it
is likely (&gt;= 50% of the time) to lead to at least one collision: 2 values being
hashed to the same bin. Using the same methodology, if if we had a hash table
of 1,000 values, hashing 38 or more values into it is likely to lead to at least
one collision. Finally, for a hash table with 1,000,000 bins, hashing 1,178 is
likely to lead to at least one collision.

We will prove that this number grows O(sqrt(N)) and even compute the actual
coefficient (which we will find as sqrt(ln(4)) = 1.17741....

Stirling's approximation for N! is N**N x e**-N x sqrt(2pixN).

If we substitute it in the formula above we get

              N   -N
            N   e    sqrt(2piN)
P = -------------------------------------
          N-k   -(N-k)                k
    (N-k)      e      sqrt(2pi(N-k)) N


			+-	 -+  (k-N-.5)
                     -k |      k  |
which simplifies to e   |  1 - -  |
                        |      N  |
                        +-       -+


So, ln(P) = -k + (k-N-.5)ln(1-k/N)

Now, ln(1+x) = x - x**2/2 + x**3/3 - x**5/5 + ... (alternating sign)

so ln(1-k/N) = -(k/N + k**2/(2N**2) + k**3/(3N**3) + ...)

so ln(P) = -k + (N-k+.5)(k/N + k**2/(2N**2) + k**3/(3N**3) + ...)

Next, assume k  