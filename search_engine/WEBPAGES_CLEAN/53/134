 ICS175winter11 
		 
			  CompSci &amp;Â Math 77B: Recommender Systems - Winter 2012  
			   Instructors: Max Welling, Dilan Gorur.  
						TA: Sungjin Ahn   
			   Goal:   
			   When you search for a book on Amazon, the website will recommend to you many other books you didn't know existed but look pretty interesting! If you perform a search on Google all sorts of advertisements will pop up on the right side of the page that are likely to catch to interest. Or, if you rate some movies on Netflix they will recommend new movies that are likely of interest to you. This personalized approach to offering interesting "items" to "users" is what we call recommender systems. The algoritms that implement these recommender systems are often called "collaborative filtering". Industry is very interested in these type of systems because they can boost their profit! The one-line idea is that by collecting data on your behavior (say online) or by asking you to rate items you have used, and by comparing you to many other people's behavior and ratings one can predict what other items you may like.    
			   In this course we will gain some hands-on experience with these recommender systems. There will be three 1 hour lectures per week where we will discuss a topic (using a few slides). However, right from the beginning you will start coding small problems in Matlab. There will also be two 1 hour lab sessions that can be used to help you finish these programming exercises and do your homework.    
			   The focus is on building recommendations systems ourselves. We will work with some real datasets. An example of such a dataset that used to  recommend jokes can be found here . You should  try the system Jester yourself here ! There are also interesting datasets that will recommend movies such as  MovieLens . A good dataset is for instance the  GroupLens dataset found here .   
			   We will learn and implement various machine learning techniques to build recommendation systems. One way to build a recommendation sstem is to find out how items (jokes, movies) are similar (based on some of their properties) and recommend items similar to the ones you said you liked before. This can be implemented using  classifiers  that map item properties to a rating value. One example is a nearest neighbor classifier that finds the most similar item with a high rating. But there are many other techniques based on matrix factorizations, clustering, etc. that we will learn about.   
			   Note, that this class is highyl interactive. You set the pace that is right for you! There is no fixed agenda and no exams. We want you to get an appreciation for research, and research can only be learned by doing it yourself. This class will also give you access to (funded) summer research projects.    
			   Slides &amp; Book Details:     
			   We will follow the book and slides that are online on  this iCAMP webpage . However, we will not discuss all of these topics, just a subset. We will also post Matlab exercises on this webpage.   
			    Lecture 2 slides    ;    Lecture 2 mfile    Lecture 3 mfile   Lecture 4 slides  ;  Lecture 4 mfile   Lecture 5 mfile  ;   Code to visulalize Jester Data  ;  Example Data   Lecture 6 mfile  ;  Saving and loading data example file         Lecture 7 Slides  ; Extra Slides Lecture 7  ;  Lecture 7 mfile       Lecture 8 Slides;   Lecture 8 mfile   Lecture 9 Slides ;  Lecture 9 mfile   Lecture 10 Slides ;  Lecture 10 mfile   Lecture 11 mfile      
			 
				   Kaggle Class Competition:  
				   Your personalized  Kaggle Competition on the Jester Jokes Dataset  (thank you Sungjin!)   
			 
			   Grading:  
			   You finish your homework and come to class.    
			   Resources:   
			   The lecture space Rowland Hall 421 is dedicated to iCAMP classes and houses 15 computers (Ubuntu) including 15 inch screens.       
			    
			 
				
			 
			   Homework/Exercises:    All homework assignments should be done in Matlab.   
			  Week 1:   Read this  Wikipedia Page on Recommender Systems ;                          Read this  Time Magazine Article on Recommender Systems  ; Check out some of the recommender systems below and play with them to get some understanding of how what they do.    Word-doc. with Matlab exercises      Week 2:    Read Chapters 1 and 2.1 from the course book "recommender systems"      Week 3:    Read Chapter 2.2 from the course book "recommender systems" ;     Word-doc. with Matlab exercises      Week 4:    Word-doc. with Matlab exercises      
			  Week 6:   Finish matlab exercises in this file     Lecture 11 mfile    
			 
				 
					 
						   Matlab Introductions:  
					 
				 
				    A list of tutorials including an interactive tutorial from the official site    
				   A video tutorial from the official site   
				   Tutorial by Stefan Roth   
				   Learning Matlab by doing Matlab   
				   This tutorial covers more math topics such as numerical linear algebra and numerical analysis   
				   Examples of Recomemnder Systems:  
				  1.  Amazon.com   (Search for your favorite book and look at what other books are recommended)    
						2.  Bing  or  Google  (Search for computer and view the ads that are displayed) 
						3.  Netflix  (If you have an account see how netflix recommends new movies to you) 
						4.     Jester  (Try it out and rate some jokes) 
							5.  MovieLens   
							6.  Pandora  (Try adding some songs and observe what it playes for you)         Watch  this Time Video on the Pandora Recommender System  (Then check out that Time Video webpage and observe it is recommending related videos ....) 
						7.  Y! Music    
						8  .  StumbleUpon   (Watch  this short video ) 
						9.  YouTube  (Observe how it recommends new video based on what you watched before) 
						10. amd many, many more......   
				  
				 
					 
						   Datasets  
						   1.  Yahoo's Sandbox  2.      GroupLens         3.      Jester           
						 
							 
								   
							 
						 
					 
				 
			 
		 
		        
		        
		          
		          
	</body> 