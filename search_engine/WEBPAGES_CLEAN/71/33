 CS 274A | Syllabus 
  
   
  


 
    CS 274A: Syllabus, Winter 2017    

Note: this syllabus may be updated during the quarter
 
   Week 1: January 9th
     Probability Review : random variables, conditional and
joint probabilities, Bayes rule, law of total probability,
chain;rule and factorization. Sets of random variables, the
multivariate Gaussian model. Conditional independence and graphical
models.
 
  
   Week 2: January 16th 
   No lecture on Monday (university holiday)
     Learning from Data : Concepts of models and parameters. Definition of the 
likelihood function and the principle of maximum likelihood parameter estimation. 
 

   
   Week 3: January 23rd
    Maximum Likelihood Learning : Using maximum likelihood methods to learn the 
parameters of Gaussian models, binomial, multivariate and other parametric models.
 
    Bayesian Learning : Frequentist and Bayesian views of probability. 
General principles of Bayesian estimation: prior densities, posterior densities, MAP, fully Bayesian approaches.  
  
   
   Week 4: January 30th
    Bayesian Learning : Dirichlet/multinomial and Gaussian examples. Predictive densities, model selection, model averaging. 
 
    Sequence Models : Learning with Markov models.
  
   
   Week 5: February 6th
    Regression Learning : Linear models. Normal equations. Systematic and stochastic components. 
Parameter estimation methods for regression.   
 
    Midterm Exam  during Wednesday's class (in-class, closed-book)
  
   
   Week 6: February 13th
    Regression Learning : Maximum likelihood and Bayesian interpretations.
 
    Classification Learning : Likelihood-based approaches and properties of objective functions. 
Logistic regression and neural network models. 
  
   
   Week 7: February 20th
   No lecture on Monday (university holiday)
 
    Bias-Variance Trade-offs : The bias-variance trade-off. Overfitting and model complexity. 
  
   
   Week 8: February 28th
    Classification Learning : Bayes rule, classification boundaries, discriminant functions,  
optimal decisions, Bayes error rate, Gaussian classifiers.
 
    Unsupervised Learning : Mixtures of Gaussians and the associated EM algorithm. 
 K-means clustering. Mixtures of conditional independence models.  
  
   
   Week 9: March 6th
    The EM Algorithm : Applications to text data. Underlying theory of the EM algorithm.
  
   
   Week 10: March 13th
    State-Space Temporal Models : Hidden Markov models.
 
    Sampling Methods : Importance sampling, Gibbs sampling, and related ideas.  
  
   
   Finals Week:  
    Final exam , in class, Monday March 20th, in class (ICS 180).
  
   

  </body> 