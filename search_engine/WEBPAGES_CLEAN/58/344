  "-//W3C//DTD HTML 4.01 Transitional//EN\"&gt;

 
UCI Machine Learning Repository: Car Evaluation Data Set: Support   
		                Center for Machine Learning and Intelligent Systems   
	 
	 
		 
		 About  
		 Citation Policy  
		 Donate a Data Set  
		 Contact 
		 

		  
		   Repository  
		  Web  
		          
		   
		 
		


		    View ALL Data Sets    
		  
      


      
	 
	  Car Evaluation Data Set  

		
		   Below are papers that cite this data set, with context shown.
		Papers were automatically harvested and associated with this data set, in collaboration with  Rexa.info . 
		   Return to Car Evaluation data set page .
		      Qingping Tao Ph. D.  MAKING EFFICIENT LEARNING ALGORITHMS WITH EXPONENTIALLY MANY FEATURES . Qingping Tao A DISSERTATION Faculty of The Graduate College University of Nebraska In Partial Fulfillment of Requirements. 2004.    (T 0 = n 2 and T s =10n 2 ). M - Metropolis, G - Gibbs, MG - Metropolized Gibbs, PT - Parallel Tempering, BF - Brute Force. Data Sets iris  car  breast cancer voting auto annealing n 4 6 9 16 25 38 M 5.3 � 2.1 1.7 � 0.831.5 � 5.05.0� 2.1 12.8 � 7.5 1.0 � 0.7 G 6.7 � 3.81.9 � 0.8 30.9 � 5.5 5.0 � 2.415.6 � 7.80.6 � 0.5 MG 6.0 � 1.7       Daniel J. Lizotte and Omid Madani and Russell Greiner.  Budgeted Learning of Naive-Bayes Classifiers . UAI. 2003.    at 50 purchases is better than its performance at 50 when the budget is set at 300. Other policies do not take the budget into account. We have observed the same overall patterns on several other datasets that we have tested the policies on so far   CAR   DIABETES, CHESS, BREAST): the performance of SFL is superior or comparable to the performance of other policies, and Biased-Robin is the best       Jianbin Tan and David L. Dowe.  MML Inference of Decision Graphs with Multi-way Joins and Dynamic Attributes . Australian Conference on Artificial Intelligence. 2003.    - giving a total 10x10=100 tests.  car  Evaluation data set: The  car evaluation  data set from the UCI repository [1] was generated from an underlying decision tree model. There are 1728 instances with four output classes in the set. Each data item has 6       Marc Sebban and Richard Nock and St�phane Lallich.  Stopping Criterion for Boosting-Based Data Reduction Techniques: from Binary to Multiclass Problem . Journal of Machine Learning Research, 3. 2002.    (Balance, Echocardiogram, German, Horse Colic, Led, Pima and Vehicle) see important improvements, ranging from 1% to } 5%. In contrast, only one dataset sees significant accuracy decrease   Car   96.0% vs. 93.9%). 9. Conclusions and Future Research This paper explores a method for prototype selection based on boosting, and gives statistical criteria       Nikunj C. Oza and Stuart J. Russell.  Experimental comparisons of online and batch versions of bagging and boosting . KDD. 2001.    for which we ran the bagging and boosting algorithms with decision trees was the  Car  Evaluation dataset from the UCI Repository. Figure 5 shows the learning curve. Batch and online bagging with decision trees perform almost identically (and always significantly better than a single decision tree).       Iztok Savnik and Peter A. Flach.  Discovery of multivalued dependencies from relations . Intell. Data Anal, 4. 2000.    which were used in the experiments are available at UCI Machine learning repository [10]. In the case of the datasets  Car   Bupa and Abalone we use randomly selected subsets of the original datasets. For each experiment we specify the name of the relation (dataset) r(R), the number of tuples in relation jrj, the       Marc Sebban and Richard Nock and Jean-Hugues Chauchat and Ricco Rakotomalala.  Impact of learning set quality and size on decision tree performances . Int. J. Comput. Syst. Signal, 1. 2000.    in detail. Actually, we note that for 3 datasets   Car   Pima and Xd6),thedecisiontreeisso reduced that no rule is induced, resulting in a decision rule in favor of the majority class in LS. This explains why accuracy falls much, respectively       Jie Cheng and Russell Greiner.  Comparing Bayesian Network Classifiers . UAI. 1999.    person makes over 50K a year. The discretization process ignores "fnlwgt" (which is one of the 14 attributes). We therefore omit "flnwgt" and use the remainder 13 attributes in our experiments.  car  dataset:  car evaluation  based on the six features of a car. Chess: chess end-game result classification based on board-descriptions. Flare: classifying the number of times of occurrence of certain type of       Daniel J. Lizotte and Omid Madani and Russell Greiner.  Budgeted Learning, Part II: The Na#ve-Bayes Case . Department of Computing Science University of Alberta.    SFL at 50 queries is better than its performance at 50 when the budget is set at 300. Other policies do not take the budget into account. We have observed the same overall patterns on several other datasets that we have tested the policies so far   CAR   DIABETES, CHESS, BREAST): the performance of SFL is superior or comparable to the performance of other policies, and BIASED ROBIN is the best algorithm       Huan Liu.  A Family of Efficient Rule Generators . Department of Information Systems and Computer Science National University of Singapore.    Equipped with the above two evaluation measures, we can conduct some experiments to empirically compare the four versions of rule generators, and with other known methods [13, 22]. A. Method One data set -  CAR  will be used to show the differences between the versions of our rule generator, and to compare with the results reported in [22] since they have done some comparison with other methods such       Zhiqiang Yang and Sheng Zhong and Rebecca N. Wright.  Privacy-Preserving Classification of Customer Data without Loss of Accuracy . Computer Science Department, Stevens Institute of Technology.    of size d, then the number of joint frequencies that need to be counted is exponential in m. In some cases we have small m and d and thus we can still achieve reasonable overhead. For example, the data set of  Car  Evaluation Database from UCI repository [BM98] has six nominal attributes: buying, maint, doors, persons, lug boot and safety, and the class attribute has a domain of size four. For such 9 a       Jos'e L. Balc'azar.  Rules with Bounded Negations and the Coverage Inference Scheme . Dept. LSI, UPC.    is analyzed and proved to have reasonable algebraic properties; and examples are given where, from a single rule, many other rules can be found through this scheme. 3. Experiments The  Car  Evaluation Dataset [BR] in the UCI repository [BM] has seven categorical columns (six attributes and a class), which we preprocessed into a single boolean attribute per value of each categorical attribute, thus       Shi Zhong and Weiyu Tang and Taghi M. Khoshgoftaar.  Boosted Noise Filters for Identifying Mislabeled Data . Department of Computer Science and Engineering Florida Atlantic University.    in Table 1. Overall, BBF-I significantly outperforms BBF-II, except for low ( 20%) noise levels for the adult,  car   and nursery datasets. The reason BBF-II performs poorly may be that too many clean instances are weighted low. The noise filter constructed in the next round loses strong support from clean data instances, which are       Hyunwoo Kim and Wei-Yin Loh.  Classification Trees with Bivariate Linear Discriminant Node Models . Department of Statistics Department of Statistics University of Tennessee University of Wisconsin.    maximum simplification, where the root node is not split. The second example shows how collinearity among predictors can creates difficulties for other classification tree methods. 4.1  Car  Data This data set is from Lock (1993). It contains specifications for 93 new car models for the 1993 year. We use the type of car (small, sporty, compact, midsize, large, and van) as the class variable. The predictor       Daniel J. Lizotte.  Library Release Form Name of Author . Budgeted Learning of Naive Bayes Classifiers.    than when it is assumed to be 300 (i.e., when it looks ahead farther than it should). Other policies do not take the budget into account. We have observed the same overall patterns on several other datasets that we have tested the policies on so far   CAR   DIABETES, CHESS, BREAST): the performance of SFL is superior or comparable to the performance of other policies, and Biased-Robin is the best 36       Nikunj C. Oza and Stuart J. Russell.  Online Bagging and Boosting . Computer Science Division University of California.    ITI online algorithm [14]; batch and online Naive Bayes algorithms are essentially identical. To illustrate the convergence of batch and online learning, we experimented with the  Car  Evaluation dataset from the UCI Machine Learning Repository [2]. The dataset has 1728 examples, of which we retained 346 (20%) as a test set and used 200, 400, 600, 800, 1000, 1200, and all the remaining 1382 examples  


	      Return to Car Evaluation data set page .


     Supported By:  
           
           In Collaboration With:  
           
   
 
 About   || 
 Citation Policy   || 
 Donation Policy   || 
 Contact   || 
 CML 
 
 





    