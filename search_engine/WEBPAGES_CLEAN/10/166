  "-//W3C//DTD HTML 4.01 Transitional//EN\"&gt;

 
UCI Machine Learning Repository: Chess (King-Rook vs. King-Knight) Data Set: Support   
		                Center for Machine Learning and Intelligent Systems   
	 
	 
		 
		 About  
		 Citation Policy  
		 Donate a Data Set  
		 Contact 
		 

		  
		   Repository  
		  Web  
		          
		   
		 
		


		    View ALL Data Sets    
		  
      


      
	 
	  Chess (King-Rook vs. King-Knight) Data Set  

		
		   Below are papers that cite this data set, with context shown.
		Papers were automatically harvested and associated with this data set, in collaboration with  Rexa.info . 
		   Return to Chess (King-Rook vs. King-Knight) data set page .
		      Manuel Oliveira.  Library Release Form Name of Author: Stanley Robson de Medeiros Oliveira Title of Thesis: Data Transformation For Privacy-Preserving Data Mining Degree: Doctor of Philosophy Year this Degree Granted . University of Alberta Library. 2005.    Pumsb (d o = 74); (b) The error produced on the dataset  Chess  (d o =37). ......................119 7.4 The error produced on the dataset Pumsb over vertically partitioned data. . . 121 List of Tables 6.1 Thread of selecting the attributes on the party k       Ira Cohen and Fabio Gagliardi Cozman and Nicu Sebe and Marcelo Cesar Cirelo and Thomas S. Huang.  Semisupervised Learning of Classifiers: Theory, Algorithms, and Their Application to Human-Computer Interaction . IEEE Trans. Pattern Anal. Mach. Intell, 26. 2004.    EMTAN can sometimes improve performance over TAN with just labeled data (Shuttle). With the  Chess  dataset, discarding the unlabeled data and using only TAN seems the best approach. We have compared two likelihood based structure learning methods (K2 and MCMC) on the same datasets as well [34], showing       Marcus Hutter and Marco Zaffalon.  Distribution of Mutual Information from Complete and Incomplete Data . CoRR, csLG/0403025. 2004.    600 900 1200 1500 1800 2100 2400 2700 3000 Instance number Prediction accuracy   Chess   FF F Figure 2: Comparison of the prediction accuracies of the naive Bayes with filters F and FF on the Chess data set. The gray area denotes di�erences that are not statistically significant. The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to       Douglas Burdick and Manuel Calimlim and Jason Flannick and Johannes Gehrke and Tomi Yiu.  MAFIA: A Performance Study of Mining Maximal Frequent Itemsets . FIMI. 2003.    0.1 0.12 Min Sup (%) Time (s) NONE ADAPTIVE Compression on BMS-WebView-2 10 100 1000 10000 0 0.01 0.02 0.03 0.04 0.05 0.06 Min Sup (%) Time (s) NONE ADAPTIVE Figure 8. Compression on more sparse datasets Compression on  Chess  1 10 100 1000 0 5 10 15 20 25 30 35 Min Sup (%) Time (s) NONE ADAPTIVE Compression on Pumsb 10 100 1000 10000 0 10 20 30 40 50 60 70 Min Sup (%) Time (s) NONE ADAPTIVE       Tanzeem Choudhury and James M. Rehg and Vladimir Pavlovic and Alex Pentland.  Boosting and Structure Learning in Dynamic Bayesian Networks for Audio-Visual Speaker Detection . ICPR (3). 2002.    accuracy. We compare its performance to both standard structure learning and boosted parameter learning on a �xed structure. We present results for speaker detection and for the UCI   chess   dataset. 1. Introduction Human-centered user-interfaces based on vision and speech present challenging sensing problems in which multiple sources of information must be combined to infer the user's actions       Marco Zaffalon and Marcus Hutter.  Robust Feature Selection by Mutual Information Distributions . CoRR, csAI/0206006. 2002.    The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the  Chess  data set. The maximum di�erence in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,       Michael G. Madden.  Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm . CoRR, csLG/0211003. 2002.    were selected that had these characteristics and that were not very small. The datasets are listed in Table 1. Dataset #I #A  Chess  (King &amp; Rook vs King &amp; Pawn) 3196 32 Wisconsin Breast Cancer Diagnosis 699 9 LED-24 (17 irrelevant attributes) 3200 24 DNA: Splice Junction Gene Sequences       James Bailey and Thomas Manoukian and Kotagiri Ramamohanarao.  Fast Algorithms for Mining Emerging Patterns . PKDD. 2002.    (Vehicle, Waveform and Letter-recognition) though. Analysis of the vehicle and  chess  datasets aid in explaining this outcome (supporting figures have been excluded due to lack of space). It is clear that classification accuracy is dependent upon finding patterns that strongly discriminate       Russell Greiner and Wei Zhou.  Structural Extension to Logistic Regression: Discriminative Parameter Learning of Belief Net Classifiers . AAAI/IAAI. 2002.    discretization [FI93]. Our accuracy values were based on 5-fold cross validation for small data, and holdout method for large data [Koh95]. See [GZ02],[FGG97] for more information about these datasets. We use the  CHESS  dataset (36 binary or ternary attributes) to illustrate the basic behaviour of the algorithms. Figure 2(a) shows the performance, on this dataset, of our NB+ELR ("Na�iveBayes       Boonserm Kijsirikul and Sukree Sinthupinyo and Kongsak Chongkasemwongse.  Approximate Match of Rules Using Backpropagation Neural Networks . Machine Learning, 44. 2001.    &amp; Feng, 1990) for learning rules. Normally we used rules produced byPROGOL as the input to BANNAR. However in our experiments on the finite element mesh design and the King-Rook-King  chess  endgame datasets described below, PROGOL failed to produce a rule set within a reasonable time. In those experiments, we employed GOLEM developed by the same research group of PROGOL. We then compared the results       Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao and Limsoon Wong.  DeEPs: A New Instance-based Discovery and Classification System . Proceedings of the Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases. 2001.    (This will be explained in Section 8.3). Note that for data sets such as  chess   flare, splice, mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not require an ff. The accuracies of k-nearest neighbor and C5.0       Jie Cheng and Russell Greiner.  Learning Bayesian Belief Network Classifiers: Algorithms and System . Canadian Conference on AI. 2001.    Wrapper(multi-net) with ordering = W-MN-O, Wrapper(multi-net) with feature selection = W-MN-FS and Wrapper(multi-net) with feature selection with ordering = WMN-FS-O. The ordering for the  Chess  data set is the reversed order of the features that appear in the data set since it is more reasonable, the ordering we use for other data sets are simply the order of the features that appear in the data       Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao.  Instance-Based Classification by Emerging Patterns . PKDD. 2000.    (as explained in [10]). Note that for the datasets such as  chess   flare, nursery, splice, mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not need ff. Columns 5, 6, 7, 8, and 9 give the       Mark A. Hall.  Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning . Doctor of Philosophy at The University of Waikato. 1999.    without feature selection using merged subsets. . . . . . . . . . . . . . . . . . . . . . . . . . 115 6.9 Top eight feature-class correlations assigned by CFS-UC and CFS-MDL on the  chess  end-game dataset. . . . . . . . . . . . . . . . . . . . . . . . 116 7.1 Comparison between naive Bayes without feature selection and naive Bayes with feature selection by the wrapper and CFS. . . . . . . . . . . .       Yk Huhtala and Juha K�rkk�inen and Pasi Porkka and Hannu Toivonen.  Efficient Discovery of Functional and Approximate Dependencies Using Partitions . ICDE. 1998.    out of memory. Table 2 shows performance results for TANE/MEM in the approximate dependency discovery task, for different thresholds ''. Results for the Hepatitis, Wisconsin breast cancer, and  Chess  data sets are also presented graphically in Figure 3: N '' =N 0 stands for the number of approximate dependencies found relative to the case for functional dependencies; similarly, Time '' =Time 0 denotes       Adam J. Grove and Dale Schuurmans.  Boosting in the Limit: Maximizing the Margin of Learned Ensembles . AAAI/IAAI. 1998.    that this depends crucially on the base learner always being able to find a sufficiently good hypothesis if one exists; see Section 5 for further discussion of this issue. 8 However, for some large data sets,  chess  and splice, we inverted the train/test proportions. FindAttrTest Adaboost LP-Adaboost DualLPboost Data set error% win% error% margin error% win% margin error% win% margin Audiology 52.30       Ron Kohavi.  Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid . KDD. 1996.    which is a Bayes network restricted to a tree topology. The results are promising and running times should scale up, but the approach is still restrictive. For example, their accuracy for the  Chess  dataset, which contains high-order interactions is about 93%, much lower then C4.5 and NBTree, which achieve accuracies above 99%. Conclusions We have described a new algorithm, NBTree, which is a hybrid       Ron Kohavi and Dan Sommerfield.  Feature Subset Selection Using the Wrapper Method: Overfitting and Dynamic Search Space Topology . KDD. 1995.    in error. The execution time on a Sparc20 for feature subset selection using ID3 ranged from under five minutes for breast-cancer (Wisconsin), cleve, heart, and vote to about an hour for most datasets. DNA took 29 hours, followed by  chess  at four hours. The DNA run took so long because of ever increasing estimates that did not really improve the test-set accuracy. 7 Conclusions We reviewed the       Brian R. Gaines.  Structured and Unstructured Induction with EDAGs . KDD. 1995.    EDAG for this problem with that obtained from human  Chess  experts. Before this is done, the following section illustrates the induction of EDAGs for a simple chess problem. 2 Modeling a Simple Chess Dataset Quinlan (1979) describes ID3 models of 7 rook versus knight end game situations of increasing difficulty. The third problem involves 647 cases with 4 3-valued attributes, 3 2-valued attributes, and       Hankil Yoon and Khaled A. Alsabti and Sanjay Ranka.  Tree-based Incremental Classification for Large Datasets . CISE Department, University of Florida.    very quickly and identify a method that better suits to the application. 5.2 Experimental results with  chess  dataset The chess dataset obtained from [BKM98] contains 25,000 training records and 3,056 test records. There are 18 class values and each record consists of 6 attributes, all of which are numeric       Omid Madani and David M. Pennock and Gary William Flake.  Co-Validation: Using Model Disagreement to Validate Classification Algorithms . Yahoo! Research Labs.    unlabeled data does not tend to wildly underestimate error, even though it's theoretically possible. 3 Experiments We conducted experiments on the 20 Newsgroups and Reuters-21578 test categorization datasets 1 , and the Votes,  Chess   Adult, and Optics datasets from the UCI collection [BKM98]. We chose 1 Available from http://www.ics.uci.edu/ and http://www.daviddlewis.com/resources/testcollections/ two       M. A. Galway and Michael G. Madden.  DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm . Department of Information Technology National University of Ireland, Galway.    were selected that had these characteristics and that were not very small. The datasets are listed in Table 1. Dataset #I #A  Chess  (King &amp; Rook vs King &amp; Pawn) 3196 32 Wisconsin Breast Cancer Diagnosis 699 9 LED-24 (17 irrelevant attributes) 3200 24 DNA: Splice Junction Gene Sequences       BayesianClassifi552 Pat Langley and Wayne Iba.  In Proceedings of the Tenth National ConferenceonArtifi256 Intelligence( 42840 . Lambda Kevin Thompson.    C4 algorithm (Buntine &amp; Caruana, 1991) and an algorithm that simply predicts the modal class. The five domains, from the UCI database collection (Murphy&amp; Aha, 1992), include the ``small'' soybean dataset,  chess  end games involving aking-12 ok--126-22 wn confrontatwobiologidata set into 80% training instances and 20% testinpairs of training and test sets. The table shows the mean accuracy and 95%       Jerome H. Friedman and Ron Kohavi and Youngkeol Yun.  To appear in AAAI-96 Lazy Decision Trees . Statistics Department and Stanford Linear Accelerator Center Stanford University.    fast algorithm. The largest running time by far was for mushroom with 8.4 Sparc-10 cpu minutes per crossvalidation fold (equivalent to a run), followed by  chess  with 1.59 cpu minutes. These datasets have 8124 instances and 3196 instances, respectively. From the table we can see that simple ID3 is generally inferior, as is C4.5 without pruning. Pruning improves C4.5-NP's performance, except for       Grigorios Tsoumakas and Ioannis P. Vlahavas.  Fuzzy Meta-Learning: Preliminary Results . Greek Secretariat for Research and Technology.    from the Machine Learning Repository at the University of Irvine, California (Blake &amp; Merz, 1998). These were the adult and  chess  data sets, large enough (} 1000 examples) to simulate distributed environment. Only two domains were selected at this stage of our research to investigate the performance of the suggested methodology. The       Nikunj C. Oza and Stuart J. Russell.  Online Bagging and Boosting . Computer Science Division University of California.    AdaBoost performed significantly worse and online boosting performed marginally worse. On the Car Evaluation and  Chess  datasets, AdaBoost and online boosting performed significantly better than Naive Bayes. On the Nursery dataset, AdaBoost performed significantly better and online boosting performed marginally better. 5  


	      Return to Chess (King-Rook vs. King-Knight) data set page .


     Supported By:  
           
           In Collaboration With:  
           
   
 
 About   || 
 Citation Policy   || 
 Donation Policy   || 
 Contact   || 
 CML 
 
 





    