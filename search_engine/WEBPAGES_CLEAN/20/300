 Control Choices and Network Effects in Hypertext Systems<body>

 
 
Control Choices and Network Effects in Hypertext Systems  

  E. James Whitehead, Jr.  

  Dept. of Information and Computer Science  

  University of California, Irvine  

  ejw@ics.uci.edu  

 
Abstract 
A key element of open hypermedia systems is the provision of hypermedia
services to the multiple applications populating a typical user's computing
environment.  A reaction against monolithic hypermedia systems which
control the user interface, data, and hypermedia structure, open hypermedia
systems control the hypermedia structure, and sometimes the data, but relinquish
control over the user interface.  Gaining strength from less control,
open hypermedia systems increase hypermedia network effects by including
more applications, hence more content, in the hypermedia web.

 The World Wide Web was designed to be a single point of entry into a
distributed, multi-platform information space, and as a direct consequence
chose different control tradeoffs.  With the Web, the user interface
is controlled via the browser, but the data and hypermedia structure are
uncontrolled.  Through these control choices, and the URL, HTML, and
HTTP standards, the Web created a feedback cycle of positive network effects.
This paper examines how the core differences in control assumptions between
monolithic hypermedia systems, open hypermedia system, and the Web, lead
to different levels of network effects.
  
1. Introduction 
Hypertext systems exhibit  network effects  [Rohlfs, 1974], where
the utility of the hypertext system depends on the number of users and
amount of data in the system.  This differs markedly from traditional
goods where the utility of the good does not depend on the number of other
people who own the good.  Breakfast cereal has the same usefulness
whether 1 or a million people own it.  In contrast, 1 telephone has
no utility, and two have not much more.  But when a million, or a
billion people have telephones, the utility of a single telephone is immense.

 Network effects in hypertext systems have many causes. Hypertext system
utility can be viewed from the perspective of readers, and of data, or
content providers. From the perspective of a reader, a hypertext link between
two documents increases the usefulness of  both  documents by making
a relationship between the documents explicit, and reducing the burden
of retrieving the associated information. As the number of links increases,
the utility of documents in the system is greater than those outside the
system. For the content provider, the utility of each document in the system
is related to the number of people who read it. As more people use the
system, it is likely more people will access the content generated by a
provider.

  Simple feedback loops lead to increases in readers and content providers. 
Readers are lured to the system to take advantage of the greater utility
of content in the hypertext system.  As more readers use the system,
content providers have incentive to add more content.  Content entices
readers, readers incentivize content, and so on. Past a critical threshold,
this feedback cycle causes users to be  locked-in  to the hypertext
system as competing systems are unable to generate sufficient network effects
to supplant the dominant system. Such is the case with the World Wide Web
today.

  For monolithic hypertext systems, the goal was to create a system that
provided hypertext functionality. Open hypermedia systems and the Web went
further, wanting to provide hypermedia functionality in more open contexts
than could be addressed by the monolithic systems. Due to these differing
goals, each class of hypermedia system made different control decisions
in its architecture. Monolithic hypertext systems control the user interface,
hypermedia structure, and data, while open hypermedia systems control the
hypermedia structure, and sometimes the data, but relinquish control over
the user interface. With the Web, the user interface is controlled via
the browser, but the data and hypermedia structure are uncontrolled. 
This paper examines how these core differences in control assumptions between
monolithic hypertext systems, open hypermedia systems, and the Web, lead
to different incentive structures for readers and content providers and
hence varying levels of network effects.
  
2. Monolithic Hypertext Systems, Control Choices and Network Effects 
Ted Nelson intuitively understood the network effects of hypertext, using
utopian rhetoric to describe a global docuverse of hyperlinked information
in Literary Machines [Nelson, 1981]. Some engineers heeded Nelson's call
to arms and labored to bridge the gap between the Nelson's ideals, and
the compromises and limitations of existing computing technology. Some
worked directly on Xanadu, while others performed their own interpretations
and implementations, leading directly and indirectly to the development
of monolithic hypertext systems such as Intermedia [Yankelovich et al.,
1988], KMS [Akscyn et al., 1987], HyperCard [Apple, 1987], and StorySpace
[Bolter, 1987][Eastgate, 1991].

 Monolithic hypertext systems, motivated by a desire to keep their information
base internally consistent, and to provide a consistent user interface,
have an architecture which tightly controls the data, hypertext structure,
and user interface of the system.  Hypertext readers using these systems
have a nice user experience with fast link traversals, and no broken links. 
Content providers using the monolithic hypertext systems are required to
import data into the system, or use system specific editors. Data storage
is typically limited to a single file or database (e.g. HyperCard), network
file system, or to data storage on a local area network (e.g., KMS), limiting
the amount of data which can be accommodated by the system, and preventing
distribution of the data across a wide area network.

  The choice to control all aspects of the system leads to limited network
effects. While readers are attracted to these systems by the rich, highly
useful content type provide, the amount and variety of this content is
limited.  Due to the need to learn new editors, and because there
are relatively few initial readers, content providers do not have a lot
of incentive to provide content.  Since there are no provisions for
remote access to the hypermedia content, the population of readers is limited
to those who have access to the local file system.  Thus, though there
was sufficient initial interest from readers of these hypertexts, there
was insufficient motivation for content providers to add new information,
eventually leading to a lack of interest from readers.  No network
effects were generated.

  The need to provide incentive for content providers was noted in [Fountain
et al., 1990], which states:
  The use of hypertext and hypermedia systems is still largely
confined to the research community.  This is partly because of the
limitations of commercially available systems and partly because of the
tremendous effort required to create and maintain a hypertext system. 
These issues are compounded by the fact that currently available hypertext
packages are basically closed systems, so that if material is created in
one system it is very difficult to integrate it with material created in
another system. We believe that this is a major barrier to the growth and
development of hypertext and hypermedia applications outside the research
community. [p. 299] 
Hypercard and StorySpace provide some limited exceptions to the lack of
network effects for monolithic hypertext systems.  Since HyperCard
was freely distributed with Macintosh computers for several years, and
HyperCard players are still part of the MacOS, a sufficient base of potential
readers existed to provide incentive for development of commercial HyperCard
stacks.  The ability to neatly package a hypertext into an easily
transportable unit, the stack, also facilitated the development of commercial
HyperCard stacks. By adding commercial incentive to produce content, more
content was developed for HyperCard than if all content utility depended
solely on the number of people reading free content.  However, today
the majority of HyperCard content is educational, produced by educators
whose job is to produce content for a small collection of readers (their
students), and hence do not require the incentive of a large set of readers
to derive utility from the content.
  
 StorySpace is another interesting exception.  With StorySpace,
content providers use the system as a form of self-expression, and are
not motivated primarily by a large readership for their hypertexts, although
a commercial market for literary hypertext does exist. The self-expression
provides sufficient motivation to produce content, and the value provided
by large numbers of readers is not necessary to "prime the pump" to provide
incentive for generation of content. Also, given the artistic requirements
for total visual control over the hypertext, a monolithic hypertext system
best meets the needs of literary hypertext.
 
3. Open Hypermedia Systems, Control Choices and Network Effects 
Open hypermedia systems began as an explicit reaction against the closed
nature of monolithic hypertext systems. [Pearl, 89], which describes the
first open hypermedia system, begins with a paragraph describing the closed
nature of monolithic hypertext systems, and and follows with a paragraph
extolling the openness of Sun's Link Service. For most open hypermedia
systems (a non-exhaustive list includes DHM [Gronbaek et al., 1993], Hyperform
[Wiil, Leggett, 1992], Microcosm [Davis et al., 1992],  Multicard
[Rizk, Sauter, 1992], and Chimera [Anderson et al., 1994]), a key quality
of openness is the support of the heterogeneous tools which populate a
user's computing environment. The rationale for this requirement is generally
pragmatic: content is produced by these tools, and they are the locus of
work on the computer. To provide hypertext support to the user's environment,
hypertext must be brought to the tools, rather than bringing the output
of the tools to the hypertext system.

 By focusing on adding hypertext functionality to desktop applications,
open hypermedia systems consciously relinquish control over the user interface
for data in the hypermedia system, and accept the need for an application
launcher component to invoke applications as needed to view data after
a link traversal. Other control choices vary (for an in-depth description
of the various control tradeoffs  in open hypermedia systems, see
[Osterbye, Wiil, 1996]). Link server systems maintain control over the
hypertext structure, but also relinquish control over the data being linked,
allowing it to reside in multiple repositories. Open hyperbase systems
control both the hypertext structure and the data being linked, thus providing
greater consistency, but requiring applications to use its data repository.

  Unlike monolithic hypertext systems, some designers of open hypermedia
systems directly considered network effects. [Pearl, 1989], in the conclusion
notes:
  With an open protocol, the power of each element of a system
expands as it interoperates with others.  Open linking can make the
power of hypertext available to the world of software.  We hope to
see linking, and attendant hypertext capabilities, as much a standard part
of the computer desktop as the cutting and pasting of text are today. [p.145] 
[Davis et al, 1992], provides a call to arms in its introduction:
 The next generation of hypermedia must appear to the user as
a facility of the operating system that is permanently available to add
information linking and navigation facilities with the minimum amount of
user intervention and without subtracting any of the functionality that
was previously available. [p. 182] 
The analysis of network effects for open hypermedia systems can still be
viewed in terms of readers and content providers, but is shifted towards
considerations of tool integration because the user interface to the data
in the hypermedia system is via pre-existing tools which are generally
hypertext unaware. Complicating the analysis is the common lack of a clear
distinction between readers and content providers.  Since many open
hypermedia systems have little or no separation between reading and authoring,
readers and content providers are often the same.

 Readers are motivated to use an open hypermedia system because of the
hypertext linking between related documents. As noted above in [Pearl,
1989], hypertext linking increases the utility of each application, due
to the interoperation provided by hypertext link traversals. Content providers
have incentive to add links because they are immediately useful (i.e.,
they are in data used by the content producer), or can be traversed by
other users of the system. The ability to link together data is limited
only by the number of hypertext-aware applications. This realization motivates
the desire to provide open hypermedia services in the operating system,
since pervasive availability of hypermedia services would lead to more
hypertext-aware applications.

  Open hypermedia systems have many problems that stem directly from not
controlling the user interface and not controlling the hyperlinked data,
and these problems limit the ability to generate network effects. The editing
problem, the data versioning problem, and difficulties with user interface
consistency are noted in [Davis et al., 1992]. Add to these the difficulty
of configuration management of different versions and types of applications
across user environments, and the problem of limited screen real estate
after several applications have been launched. Finally, the lack of highly
scalable remote data access support in open hypermedia systems is also
a noted problem which has spawned much current research.  Altogether,
these issues reduce the incentives for readers, and increase the maintenance
burden for content providers. The lack of distribution support further
caps the total possible number of readers, putting an upper limit on the
potential utility of the information. However, even if global distribution
was available, the problems inherent in providing hypertext services across
widely divergent user machine and application configurations would also
limit the utility of these hypertexts for readers.
  
4. World Wide Web, Control Choices and Network Effects 
The Web began as a reaction against an information space very similar to
that being touted as ideal by the open hypermedia community. Rather than
having to use multiple programs on several computers to access information,
the Web aimed at being a unified access point for the information provided
by these programs. Slides from a 1993 presentation [Berners-Lee, 1993]
describe the concept of universal readership:
 Before W3, typically to find some information at CERN one had
to have one of a number of different terminals connected to a number of
different computers, and one had to learn a number of different programs
to access that data. The W3 principle of universal readership is that once
information is available, it should be accessible from any type of computer,
in any country, and an (authorized) person should only have to use one
simple program to access it. 
To achieve this goal, the Web made different control tradeoffs from either
monolithic or open hypermedia systems, choosing to control the user interface
(via the browser) but not controlling either the hypertext structure, or
the hypertext data.  The lack of control over the hypertext structure
and data allowed these aspects of the system to be massively decentralized.
The triad of standards, URL [Berners-Lee et al., 1994], HTTP [Fielding
et al., 1997], and HTML [Raggett et al., 1998] provided the foundation
for interoperation in a widely distributed, large-scale information space.

 With the clarity of hindsight, the Web appears optimally suited for
generating network effects.  As the 1993 talk notes:
  To allow the web to scale, it was designed without any centralized
facility. Anyone can publish information, and anyone (authorized) can read
it. There is no central control. To publish data you run a server, and
to read data you run a client. All the clients and all the servers are
connected to each other by the Internet. The W3 protocols and other standard
protocols allow all clients to communicate with all servers. 
Since the Web provided a single user interface to existing repositories
of information (a valuable interface on the early Web was to the phone
book at CERN), as well as hypertext linking from documents which supported
HTML, readers had incentive to use the system. For content providers, the
Web offers a significant barrier to entry, requiring the installation and
configuration of a Web server and, for many providers, initial or improved
connection to the Internet. Not surprisingly, the early Web was limited
by the small amount of information available, and the fact this information
was related almost entirely to high energy particle physics. Two events
in 1993 reduced the barriers to entry for both readers and content providers. 
First, the NCSA HTTP server was released, and was rapidly ported to most
current computing platforms.  Unlike the other existing server, the
CERN server, this server could be installed by any user, and did not require
super user (root) access, and this allowed installation of Web servers
without the need for securing buy-in from typically conservative computing
support organizations. Second, the release of the Mosaic browser on Unix,
Mac and PC platforms increased the base of potential users, and provided
a visually pleasing interface which increased reader's incentives for using
the system. While these two events would eventually have touched off the
frenzy of growth which categorized the Web in 1994-6, an article in the
Business section of the New York Times in December, 1993 [Markoff, 1993]
added sufficient new users to jump-start the cycle of increasing network
effects, as new readers increased the incentives for content providers,
who provided more information, leading to more readers, etc.

 By controlling the user interface, the Web is able to provide a single,
attractive, easy-to-use entry point into the system. Recognizing that a
single application cannot provide viewers for all media types, the typical
browser provides launch-only hypertext services to invoke an application
which displays the unknown media type, and plug-ins, which allow viewers
for unknown types to use the same screen real estate as the browser. If
the Web controlled the hypermedia structure, it would have led to a single
scalability choke point as increasing numbers of systems accessed the same
system for link information. By not placing control requirements on the
data displayed by the system, the Web could accommodate a wide range of
information repositories, enabling more information providers.

  Though the web has its noted drawbacks, with broken links, slow data
access, and lack of versioning support being more frequently mentioned
problems, it is notable that these problems have not disincentivized readers
or content providers sufficiently to cause them to abandon the system,
nor did they noticeably dampen the rate of adoption of the Web.
  
5. Conclusion 
This paper described a simple model of how the interaction between information
producers (content providers) and consumers (readers) leads to the generation
of network effects in hypertext systems.  Three classes of hypertext
systems, monolithic, open, and Web are analyzed from the perspective of
the control decisions embedded in their architectures, and how these control
decisions led to differing levels of network effects.

 Although preliminary, the discussion in this paper are suggestive of
several points. First, lack of control over the data in a hypermedia system,
combined with a large-scale distribution infrastructure is a key aspect
of achieving network effects, since this control choice affords large numbers
of readers. Examination of network effects from the Web and monolithic
hypermedia systems suggests that control over the user interface is a key
contributor to network effects, since it incentivizes readers, and allows
for more control over the presentation by content providers. Control over
the hypermedia structure provides a negative contribution to network effects,
since the control point limits scalability, thus capping the total number
of readers.
  
References 
[Akscyn et al., 1988] R. Akscyn, D. L. McCracken, E. Yoder, KMS: A distributed
hypermedia system for sharing knowledge in organizations,  Comm. ACM ,
31(7), 820-835, July, 1988.

 [Anderson et al., 1994] K. M. Anderson, R. N. Taylor, E. J. Whitehead,
Jr., Chimera: Hypertext for heterogeneous software environments,  Proc.
ECHT'94 , Edinburgh, Scotland, September, 1994, pages 94-107.

  [Apple, 1987] Apple Computer, Inc.,  Hypercard User's Guide , Cupertino,
California, 1987.

  [Berners-Lee, 1993] T. Berners-Lee, WorldWide Web Seminar, unpublished
slides, &lt;http://www.w3.org/Talks/General.html&gt;.

  [Berners-Lee et al., 1994] T. Berners-Lee, L. Masinter, M. McCahill,
"Uniform Resource Locators (URL)", CERN, Xerox, Univ. of Minnesota, Internet
RFC 1738, December, 1984.

  [Bolter, 1987] J. D. Bolter, M. Joyce, Hypertext and Creative Writing,
 Proc. Hypertext'87 , Baltimore, 1987, pages 41-50.

  [Davis et al., 1992] H. Davis, W. Hall, I. Heath, G. Hill, Towards an
integrated information environment with open hypermedia systems,  Proc.
ECHT'92 , Milano, Italy, November-December, 1992, pages 181-190.

  [Eastgate, 1991] Eastgate Systems, Inc.,  Storyspace  hypertext
writing environment for Macintosh computers, 1991.

  [Fielding et al., 1997] R. Fielding, J. Gettys, J. Mogul, H. Frystyk,
T. Berners-Lee, "Hypertext Transfer Protocol -- HTTP/1.1", U.C. Irvine,
DEC, MIT/LCS, Internet RFC 2068, January, 1997.

  [Fountain et al., 1990] A. M. Fountain, W. Hall, I. Heath, H. Davis,
Microcosm, An open model for hypermedia with dynamic linking,  Proc.
ECHT'90 , Versailles, France, November, 1990, pages 298-311.

  [Gronbaek et al., 1993] K. Gronbaek, J. A. Hem, O. L. Madsen, L. Sloth,
Designing Dexter-based cooperative hypermedia systems,  Proc. Hypertext'93 ,
Seattle, Washington, November, 1993, pages 25-38.

  [Markoff, 1983] J. Markoff, A free and simple computer link; enormous
stores of data are just a click away,  New York Times , v143, Wed,
Dec 8, 1993, col 3.

  [Nelson, 1981] T. H. Nelson,  Literary Machines , self-published,
1981.

  [Osterbye, Wiil, 1996] K. Osterbye, U. K. Wiil, The Flag taxonomy of
open hypermedia systems,  Proc. Hypertext'96 , Washington, DC, March,
1996, pages 129-139.

  [Pearl, 1989] A. Pearl, Sun's Link Service: A protocol for open linking,
 Proc. Hypertext'89 , Pittsburgh, Pennsylvania, November, 1989, pages
137-146.

  [Raggett et al., 1998] D. Raggett, A. Le Hors, I. Jacobs, "HTML 4.0
Specification", W3C Recommendation REC-html40-19980424, April, 1998.

  [Rizk, Sauter, 1992] A. Rizk, L. Sauter, Multicard: An open hypermedia
system,  Proc. ECHT'92 , Milano, Italy, November-December, 1992, pages
4-10.

  [Rohlfs, 1974] J. Rohlfs, A theory of interdependent demand for a communications
service,  Bell Journal of Economics  5(1), 1974, pages 16-37.

  [Wiil, Leggett, 1992] U. K. Wiil, J. J. Leggett, Hyperform: Using extensibility
to develop dynamic, open and distributed hypertext systems,  Proc. ECHT'92 ,
Milano, Italy, November-December, 1992, pages 251-261.

  [Yankelovich et al., 1988] N. Yankelovich, B. Haan, N. Meyrowitz, S.
Drucker, Intermedia: the concept and the construction of a seamless information
environment.  IEEE Computer , 21(1):81-96, January, 1988.
 </body> 