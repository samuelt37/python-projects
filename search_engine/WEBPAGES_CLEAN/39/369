 Fundamentals of Memory and Memory Management


Memory Hierarchies: Speed/Size/Cost Tradeoffs

To start, we will discuss hierarchies in memory, including tradeoffs between
their speeds, sizes, and costs. First, we will discuss the following prefixes
for sizes/speeds, which every computer scientist should be familiar with.

    kilo  x 10^3	      milli  x 10^-3
    mega  x 10^6	      micro  x 10^-6
    giga  x 10^9	      nano   x 10^-9
    tera  x 10^12	      pico   x 10^-12
    peta  x 10^15	      femto  x 10^-15
    exa   x 10^18	      atto   x 10^-18

So, 1 gigabyte is 10^9 bytes. The following might become relevant for your life

    zetta x 10^21	      zepto  x 10^-21
    yotta x 10^24	      yoxto  x 10^-24

There are three standard locations for memory: on the CPU chip (also known as
cache memory), in special memory chips (main memory), and external memory (disk
drives, DVDs, USB, etc.). Each is slower than the previous, but its size is
bigger and its cost/byte is lower. CPU chips act as a cache for main memory;
and main memory often acts as a cache for external memory (some external memory
devices, like disk drives, also have their own dedicated caches).

A book by Van Loan/Fan (Insight Through Computing) made some of these numbers
concrete.

  1 megabyte: A 500 page novel; 1 minute of MP3 music
  1 gigabyte: The human genome; 20 minute of a DVD
  1 terabyte: A University library; photos of all US Airline passengers (1 day)
  1 petabyte: The amount of text in the Library of Congress; he says that
              printing it would take 50x10^6 trees

------------------------------------------------------------------------------

External Memory/Disk Drives:

Note that disk drives/DVDs are modeled by a spinning platter that has
concentric circles that each store data; there is a "read head" that can move
between concentric circles. To read a specific word of data, the read head seeks
the correct circle and waits for the data to rotate into the correct position
under it. A typical rotation speed is 7,200 rpm. This is roughly 100
revolutions/second, meaning that it takes about 10 milliseconds to complete one
full rotation; this is called the "rotational delay". The read head takes about
the same amount of time to move into position over the correct circle (called
"seek time"). Note that a processor executing 1 billion operations per second
can execute 10 million instructions in 10 milliseconds, while the read head
seeks and the platter rotates to the needed position, so it can do a lot of
work between setting up for the read and actually receiving the data..

We will look at algorithms that measure their efficiency not by the number of
instructions executed, but by the number of times they must seek data on an
external memory device, because the time waiting for data will dominate the
time spent in the number of instructions executed.

We often talk about the terms "latency" and  "bandwidth" when discussing
memory access (and transmitting information over networks as well). Latency is
the time it takes from a request for data until the first information arrives.
Bandwidth is the throughput (data rate) starting when the first data begins to
arrive. The latency for a disk drive/DVD can be large (see the 10 millisecond
numbers above), because it involves moving something physical: the read head
and the platter; but once the read head/platter are in the right position, with
the right data is under it, the quickly spinning platter (storing data densely)
can transfer all the data on the circle very quickly. Historically, data
transfer rates on standard hard drives are ~70 megabytes per second.

Because external memory typically has high latency (a long time, measured in
machine instructions that can execute before the first piece of data arrives)
and high bandwidth (after that, lots of data can be delivered quickly), we
typically use every memory request to transfer a BLOCK of memory, not a single
word of memory. The expectation is that the subsequent information will be
needed soon (certainly the case when reading a sequential file).

So, when reading information from a file stored on a disk drive, instead of
just reading one character at a time, many characters (tens of thousands to
millions) are read and cached in main memory (or the disk drive's dedicated
cache; these caches can transfer data to memory at the rate of 3 gigabytes per
second -much faster than information can be read as the disk rotates).

So, for many subsequent character reads, the information is retrieved directly
from memory and the computer doesn't have to perform any more disk operations.
Such a cache has a special name, a "buffer". When the buffer is exhausted (all
characters read from it), the next character read initiates another block
transfer. As we saw, it might take 10 millisecond to move the read head and
wait for the data to rotate on the platter under the read head, but it will
take little time to read tens of thousands to millions more characters. Another
way to look at this: to get one character takes 10 milliseconds, but to get
100,000 characters requires only 10 + 1 = 11 milliseconds (at 100 megabytes/sec:
a round number that is a bit more,than the 70 quoted above as the transfer
rate). So, the amortized cost of reading one of the 100,000 characters is
11/100,000 milliseconds/character or about .11 microseconds/character (an
overall rate of almost 10 million characters/second). If we read 1,000,000
characters into the buffer, the rate would be 20/1,000,000 or about
.02 microseconds/character, an overall rate of 50 million characters/second).

This analysis is a bit like what we did for putting N values in an ArrayQueue,
which doubles its size. Some adds are very quick, but a few require much more
time, when the array size doubles. But if you take a look at lots of adds, the
average cost is very cheap. Likewise, when reading blocks of characters, reading
every new block takes lots of time, but most characters will then be in the
memory buffer for quick reading.

Here are some current sizes and relative speeds for CPU Chip, main, and
external memory (these are very approximate).

  CPU Chip ~1-10 Mb  10-100 times faster than main memory
  Main     ~1-10 Gb  100K - 1M times faster than external memory (latency)
  External ~1- ? Tb 

As a rule of thumb, each step up increases the size by a factor of 1,000 and
decreases the speed and cost/byte by a similar factor.

When we have analyzed algorithms earlier in this course, we have assumed that
all data is in main memory. In fact, often most of the data is on the CPU chip
memory, and its performance is often an important practical consideration for
determining the time an algorithm will take. With effective caching, often 9 of
10 memory access will occur in the CPU Chip memory, not the main memory. This
can speed up the execution by a factor of 10-100. We will briefly discuss the
interplay between CPU chip and main memory below, using CPU chip memory to cache
information from main memory.

------------------------------------------------------------------------------

Data Access patterns and the CPU Cache:

Access patterns for data often exhibits two kinds of locality.

 Temporal locality: if data is being accessed now, it is likely to be accessed
          soon in the future; for example, a loop index (or more generally, a
          cursor) is accessed frequently during the execution of a loop (its
          value is initialized, checked, and updated in each loop iteration).

 Spatial  locality: if data is being accessed now, data near it is likely to be
          accessed soon in the future; for example, if we are accessing an
          ARRAY at position i, it is likely we will access it at position i+1
          in the near future (when scanning an array, not doing a binary search
          in an array). This effect is similar, but not quite as pronounced in
          linked lists/trees whose nodes were allocated at a similar time (and
          therefore initially in memory locations that are close by).

Scanning all the values in an array, using an index variable, exhibits both
temporal (the index variables) and spatial (the array elements) locality. If
we can rewrite our program (or write it in machine code) so that it can 
completely fit in CPU chip memory, it can run much faster than a slightly
bigger amount of code that cannot fit in CPU chip memory.

The following algorithm is implemented in hardware. It is used whenever the
CPU needs to access data. Here we use the term cache for the CPU Chip memory.
The cache starts out empty.

 1) If the data is already in the cache, use it

 2) If the data is not already in the cache
      a) Retrieve it (and other data near it: some block of memory)
         As with external memory, there is high latency to get the data, but
         there is high bandwidth to transfer a block of data from main memory
         to the cache, so it accesses/transfers blocks of data
      b) If the cache is not full, add the new block of data to it
      c) If the cache is full, determine which block of data to remove and
           add the new block of data to replace it

In the future, all memory addresses in the cache can be accessed quickly; an
address outside the cache must go through the process above.

We need a policy dictating which old data block to remove from a filled
cache. Three standard and well-studied policies are Random, First in first out
(FIFO), and Least Recently Used (LRU). Any policy must be fairly simple,
otherwise it could not be directly implemented in hardware (because of the
speeds needed, cache replacement algorithms must be implemented in hardware).

The idea is to leave inside the cache any data that is expected to be accessed
soon in the future. Random does not bring anything relevant into the decision,
but it is simple/cheap to implement (no extra storage). FIFO seems a reasonable
strategy: if something was brought in a long time ago, it is less likely to be
used compared to something that was brought in more recently (it requires only
a simple queue to keep track of which block to replace next). But, LRU gets
more to the heart of temporal locality: if something has been used recently, it
is more likely to be used in the near future (regardless of when it was
initially used, which is what FIFO monitors). While LRU is harder to implement
in hardware (it uses something like a priority queue), it can be implemented
there and it is a better predictor of what to remove and what to leave in the
cache.

Because the time needed to locate and transfer a block of data is large (while
waiting for the data to arrive, the computer could execute many instructions),
choosing a replacement policy like LRU that is less efficient (takes more time
to determine what block to remove) but better (determines more accurately which
block won't be used in the future) is likely to provide better performance
overall.

The concept of "prefetching" works for Main-&gt;CPU or External-&gt;Main memory.
If as programmers we know that some data is expected to be used in the near
future (but not needed yet) we can prefetch it (touch it so that it loads into
the cache). Then, while the computer is doing other things, before the data is
actually needed, it will be brought from the slower to the faster memory. 

The maintenance of caches is an important part of chip design. Cache design
becomes even more interesting when multiple cores/processors access the same
memory. For example, if 4 CPUs each cache some main memory that they share,
and one CPU changes the value (in its cache) the other CPUs that are caching
that same memory have to be updated (and main memory as well, eventually).

When new cache mechanisms are proposed, they are often evaluated by using
previously collected "memory traces" showing which memory locations actual
"important" programs generate, and determining how well the caching mechanism
works in these cases. Such memory traces can constitute billions, trillions
(even more) of memory references of a running program; recall machines can
execute billions of operations per second.

Likewise, using the concept of "virtual memory" we can consider main memory to
act as a cache for external memory. Using virtual memory, we can solve huge
problems by just pretending that the computer's memory is as large as its
disk-drive's memory (terabytes not gigabytes). Then we use the main memory
as a cache for external memory (just as we discussed above using a CPU
Chip's memory as a cache for main memory, including replacement policies).

Using virtual memory we can "easily" solve problems that do not fit into a
computer's main memory, but unless the data structures and algorithms
processing the data structures exhibit strong temporal/spatial locality, the
run time can be enormously larger (thousands to millions of time longer, as
data is shuttled between main and externall memory). In the next two lectures
we will discuss data structures and algorithms for fast searching and sorting,
when using huge amounts of external memory.

Gordon Bell (a famous computer designer) has written a book called "Total
Recall: How the E-Memory Revolution will change Everything" (Dutton, 2009). In
the book he posits that in the future, everyone can have everything that they
ever see and hear (e.g., every conversation that they have), every web-page
that they look at, etc. stored in memory and indexed for retrieval. Here is a
quote from page  9, early in the book.

  In fact, digital storage capacity is increasing faster than our
  ability to pull information back out. Once upon a time, you had
  to be extremely judicious and stingy about which pieces of data
  you hung on to. You had to be thrifty with your electronic pieces
  of information, or bits, as we call them. But starting around 2000
  it became trivial and cheap to sock away tremendous piles of data.
  The hard part is no longer deciding what to hold on to, but how to
  efficiently organize it, sort it, access it, and find patterns and
  meaning in it. This is a primary challenge for the engeineers
  developing the software that will fully unleash the power of Total
  Recall.

Basically, Moore's Law (http://www.intel.com/technology/mooreslaw/) postulated
by Gordon Moore (Intel) says that the number of transistors in a given area
will double every 18 months. This typically translated into computer speed
doubling as well, but not now. It requires too much power to speed up
computers. So instead, we use the extra transistors to create more cache memory
and more cores (CPUs) on a single chip. They all run at a "slow" speed, but if
programmed correctly, to work together, they can accomplish as much as a faster
chip. How to coordinate cores is still a problem (some say the biggest practical
problem facing computing today).

External memory is still growing at a slightly faster pace than predicted by
Moore's law; typically every couple of years you can buy twice the amount of
external memory for about the same price (with no speed degradation, but also
not a lot of speed improvements). Solid state external memory is gaining (cost
and performance) on hard disk drives.

------------------------------------------------------------------------------

Stacks and Heaps:

Most computer programming languages use memory in two special ways: as a stack
and as a heap (NOT the same kind of binary heaps used for efficient priority
queues; here the same name is used for something very different). Main memory
is really just a giant array of words. A 32 bit word stores an int or a
reference; it can also be divided into four, 8-bit bytes, where each byte can
store a single ASCII character.

Think of all available memory (once the program has been stored in memory) as
being divided between stacks and heaps, with the stack on the left growing 
towards the right, and the heap on the right growing towards the left

 Memory
+---------+----------------------+
| program | Stack -&gt;       