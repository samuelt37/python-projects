 Christian Medeiros Adriano<body>

 
     


  
   Christian M. Adriano   
  

      
  
    Home    
     
    Blog    
     



     M.Sc., PMP
 ACM and IEEE Member    adrianoc [at] uci.edu    christian.adriano[at]ieee.org    Linked-in   
 GitHub   
 Google
Scholar    Professional Service   SEWORLD  Moderator 2014 - 2016   


     
 


  Exploring Microtask Crowdsourcing as a Means of Fault Localization     As a graduate student, I explored the use of template questions as a means to partition software debugging into small tasks (microtasks). Each microtask consisted of one question about a possible relationship between a particular source code fragment (i.e., a few lines of code) and the cause of a software failure. My findings showed that a crowd of programmers can correctly distinguish questions that cover lines of code containing a fault from questions that do not.  

 Microtasks were executed on a platform (CrowdDebug) that automates a four-steps workflow: programmer qualification, microtask generation, answer collection, and fault localization. Programmers who pass a program comprehension test are qualified to take microtasks.  Microtasks are automatically generated from a set of template questions. Each concrete question asks the programmer whether a particular source code fragment is related to the cause of a unit test failure. The fault localization step consists of identifying the lines of code that most probably contain the fault. This is accomplished by a set of predictive analytics and data filters applied to the collected answers.  

 My findings were based on two large-scale experiments, each involving five hundred programmers who collective answered more than five thousand answers about real bugs from popular open source software projects. Experiment participants were invited through Mechanical Turk (MTurk), which means that interested participants had to accept a task in MTurk before being able to take a qualification test and perform microtasks on CrowdDebug. This enabled me to reach out thousands of MTurk workers with a broad range of programming experience and who provided valuable information about their demographics.  

 Through my experiments, I learned that programmers could collectively identify simple faults in popular open source software when asked targeted questions about small code fragments. Programmers distinguished faulty from non-faulty code fragments with 79% precision and 68% recall. However, I also uncovered limitations in terms of cost and speed. In response to that, I investigated a set of heuristics that could help identifying the set of answers that maximize the effectiveness of fault localization. In doing so, I identified, within the original crowd of programmers, an additional 30 distinct subgroups of programmers (subcrowds) who have also successfully located all the faults. Remarkably, five of these subcrowds presented precision values higher than 79%. 

 
 
 
 

</body> 