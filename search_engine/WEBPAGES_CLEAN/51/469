 Analysis of Algorithms (Complexity Classes and Big-O Notation)


Analysis of Algorithms is a mathematical area of Computer Science in which we
analyze the resources (mostly time, but sometimes space) used by algorithms to
solve problems. An algorithm is a precise procedure for solving a problem,
written in any notation that humans understand (so that we can carry-out the
algorithm): if we write an algorithm as code in some programming language, then
a computer can execute it too. The analysis we do should be independent of any
technology: e.g., compiler or processor.

The main tool that we use to analyze algorithms is big-O notation: it means
"bounded above by growth on the order of". We use big-O notation to bound from
above the performance of an algorithm by placing it in a complexity class (most
often based on its WORST-CASE behavior -but sometimes on its AVERAGE-CASE
behavior) when solving a problem of size N: we will learn how to characterize
the size of a problem, which is most often as simple as "N is the number of
values in" an array, file, vector, linked list, etc. Once we know the
complexity class of an algorithm, we have a good handle on understanding its
actual performance (within certain limits). Thus, in AA we don't necessarily
compute the exact resources needed, but typically an approximate upper bound
on the resources, based on just the problem size (not the details of the data).

------------------------------------------------------------------------------

Getting to Big-O Notation: Throwing away Irrelevant Details

Here is a simple C++ function for computing the maximum of a array (or throwing
and exception if there are no values in the array).

int maximum(int a[], int N) { //N is the length of the array
    if (N == 0)
      throw "empty array";
    int answer = a[0];
    for (int i=1; i answer)
        answer = a[i];
    return answer;
}

Often, the problem size is the number of values processed: e.g., the number of
values in an array, file, linked list, tree, etc. But we can use other metrics
as well: e.g., it can be the count of number of digits in an integer value, when
looking at the complexity of multiplication based on the size of the numbers.
Thus, there is no single measure of size that fits all problems: instead, for
each problem we try to choose a measure that makes sense and is natural for
that problem.

C++ compiles functions like maximum into a sequence of machine language
instructions that the computer executes. To solve a problem, the computer
always executes an integer number of instructions. For simplicity, we will
assume that all instructions take the same amount of time to execute. So to
compute the amount of time it takes to solve a problem is equivalent to
computing how many instructions the computer must execute to solve it (which
depends on the compiler technology we are using), which we can divide by the
number of instructions/second the machine executes to compute the time taken
(which changes as technology changes).

Again, we typically look at the worst case behavior of algorithms. For maximum,
the worst case occurs if the array value are stored in increasing order. In
this case, each new value examined in the array will be bigger than the
previous answer, so the if statement's condition will always be true, which
always requires executing the code that updates the answer. If any value was
lower, it wouldn't have to update the answer and thus would execute fewer
instructions and take less time to execute.

It turns out that for an array of N values, on a certain C++ compiler for a
certain processor, the computer executes 14N + 9 instructions in the worst case
for this function. You may need to know more CS than you do at this time to
determine this formula, but you will get there by ICS 51; and if you have taken
ICS-51, you should understand the machine instructions that such a program
might be translated into.

A simple way to think about this formula is that there are 14 computer
instructions that are executed each time the body of the for loop is executed,
and 9 instructions that are executed only once: they deal with starting
and terminating the loop and the entire function. We can say I(N) = 14N + 9 for
the worst case of the maximum function, where I(N) is the number of
instructions the computer executes when solving a problem on an array with N
values. Or to be more specific we would write Imaximum(N) = 14N + 9.

I would like to argue now that if simplify this function to just Imaximum(N)
= 14N, we get a simpler function; although we lose some information, we don't
lose much. Specifically, as N gets bigger (i.e., we are dealing with very big
problems - the kinds computers are used to solve), 14N and 14N+9 are relatively
close: as N gets bigger the loop accounts for most of the instructions executed)
Let's look at the result of this function vs. the original as N gets for values
of N increasing by a factor of 10.

    N     |   14N + 9  |    14N     | error: (14N+9 - 14N)/(14N+9) as a % of N
 ---------+------------+------------+---------------------------
        1 |         23 |         14 |   61%         or 39%       accurate
       10 |        149 |        140 |    6%            94%       accurate
      100 |       1409 |       1400 |     .6%          99.4%     accurate
     1000 |      14009 |      14000 |     .06%         99.94%    accurate
     ...
1,000,000 | 14,000,009 | 14,000,000 |     .00006%      99.99994% accurate
     ...

Each line shows the % error of computing 14N when the true answer is 14N + 9.
So by the time we are processing an array of 1,000 values, using the formula
14N instead of 14N+9 is 99.94% accurate. For computers solving real problems,
an array of 1,000 values is tiny: an array of millions is more normal. For
1,000,000 values 14N is off by just 9 parts in 14 million. So the 9 doesn't
affect the formula much. Almost all the time is spent in the loop.

Analysis of Algorithms really should be referred to as ASYMPTOTIC Analysis of
Algorithms, as it is mostly concerned with the performance of algorithms as
the problem size gets very big (N -&gt; infinity). We see here that as N-&gt;infinity
14N is a better and better approximation to 14N+9: dropping the extra 9 becomes
less and less important.

A simple function for sorting is the following. This is much simpler than the
real sort method in C++ (and the simplicity results in the function taking much
more time for large N, but it is a good starting point for understanding
sorting now). If you are interested in how this function accomplishes sorting,
hand simulate it working on an array of 5-10 values (try arrays with increasing
values, decreasing values, values in random order): basically each execution of
the outer loops mutates the array so that the next value in the array is in the
correct position. We will spend a few lectures studying sorting more formally
towards the end of the quarter.

void sort(int a[], int N) { //N is the length of the array
  for (int base=0; base a[check])
         std::swap(a[base], a[check]);
}

It turns out that for an array of N values, the computer executes 8N^2 + 12N + 6
instructions in the worst case for this function. The outer loop executes N
times and inner loop on average executes N/2 times (when base is 0 it executes
N-1 times; when base is N-1 it executes 0 times), so the if statement in the
inner loop is executed a quadratic number of times. So Isort(N) = 8N^2+12N+6
for the worst case of the sort function, where Isort(N) is again the number of
instructions that the computer executes. Or to be more specific we would write
Isort(N) = 8N^2 + 12N  + 6.

I would like to argue in the same way that if simplify this function to just
Isort(N) = 8N^2, we get a much simpler function but we have not lost much
information. Let's look at the result of this this function vs. the original as
N gets bigger and bigger

    N     | 8N^2+12N+6 |      8N^2   | error: (12N+6)/(8N^2+12N+6) as a % of N
 ---------+------------+-------------+---------------------------
        1 |         26 |           8 |   70%      or 30%    accurate
       10 |        926 |         800 |   14%         86%    accruate
      100 |     81,206 |      80,000 |    1.5%       98.5%  accurate
     1000 |  8,012,006 |   8,000,000 |     .15%      99.85% accurate

So by the time we are processing an array of 1,000 values, using the formula
8N^2 instead of 8N^2 + 12N + 6 is 99.85% accurate. For 1,000,000 values
(10^6), 8N^2 is 8*10^12 so 8N^2 + 12N + 6 is 8*10^12 + 12*10^6 + 6; the
simpler formula is 99.999985% accurate. Note the difference in the powers of
10 for the first and second terms: 10^12 and 10^6.

CONCLUSION (though not proven): If the real formula I(N) is a sum of a bunch of
terms, we can drop any term that doesn't grow as quickly as the most quickly
growing term. As N-&gt;infinity, we can neglect the effects from all these lower
order terms without losing much accuracy. In computing the maximum, the linear
term 14N grows more quickly than the next term, the constant 9, which doesn't
grow at all (as N grows) so we drop the 9 term. In sorting, the quadratic term
8N^2 grows more quickly than the next two terms, the linear term 12N and the
constant 6, so we drop the 12N and 6 terms.

    In fact note that we can prove that the Limit as N-&gt;infinity of 12N/8N^2 = 
    3/(2N) -&gt; 0, which means we can discard the 12N term as growing more slowly
    than the 8N^2 term. Similarly, the Limit as N-&gt;infinite of 6/8N^2 -&gt; 0.

The result is a simple function (constant times some function of N) that is
still an accurate approximation of the number of computer instructions executed
for arrays of various large sizes.

    But also note, if the formula were something like N^2 + 1,000,000N (with the
    coefficient of N one million times greater than the coefficient of N^2), at
    N = 1,000,000 the N^2 term is the same size as 1,000,000N term, so the
    percent error would be 50%. Of course if we increase N to a billion or
    trillion, N^2 becomes much bigger. In most algorithms, the coefficients
    are of the same magnitude, so throwing away lower-order terms is still
    accurate for moderate-sized N.

We now will explain a rationale for dropping the constant in front of N (in
Imaximum) and N^2 (in Isort), and classifying these algorithms as O(N) bounded
by linear growth, and O(N^2) bounded by quadratic grow5th. Again O means
"bounded by growth on the order of", so O(N) means bounded by growthon the
order of N and O(N^2) means bounded by growth on the order of N^2.

1) If we assume that every instruction in the computer takes the same amount
   of time to execute. Then the time taken for maximum is about 14N/speed and
   time for sort is about 8N^2/speed. We should really think about these
   formulas as (14/speed)N and (8/speed)N^2. We know the 14 and 8 came from
   the number of instructions inside loops that C++ needed to execute: but a
   different C++ compiler (or a different language) might generate a different
   number of instructions and therefore a different constant. Thus, this number
   is based on technology, and we want our analysis to be independent of
   technology. And, of course, "speed" changes based on technology too.

Since we are trying to come up with a "science" of algorithms, we don't want
our results to depend on technology, so we are also going to drop the constant
in front of the biggest term as well. For the reason explained above (relating
to instructions generated by C++ and the speed of the machine), this number
is based solely on technology.

Here is another justification for not being concerned with the constant in
front of the biggest term.

2) The fundamental question that we want answered about any algorithm is, "how
   much MORE resources does it need when solving a problem TWICE AS BIG". In
   maximum, when N is big (so we can drop the +9 without losing much accuracy)
   the ratio of time to solve solve a problem of size 2N to the time to solve a
   problem of size N is easily computed:

    Imaximum(2N)     14(2N)
   -------------- ~ -------- ~ 2
    Imaximum(N)      14 N

   The ratio is a simple number (no matter how many instructions are in the
   loop, since the constant 14 appears as a multiplicative factor in both the
   numerator and denominator, and cancels itself out).  So, we know for this
   code, if we double the size of the array, we double the number of
   instructions that maximum executes to solve the problem, and thus double the
   amount of time (for whatever the speed of the computer is).

   Thus, the constant 14 is irrelevant when asking this "doubling" question. 

   Likewise, for sorting we can write

    Isort(2N)     8(2N)^2
   ----------- ~ ---------- ~ 4
    Isort(N)      8 N^2

   Again, the ratio is a simple number, with the constant (no matter what it
   is), disappearing.  So, we know for this code that if we double the size of
   the array, we increase by a factor of 4 the number of instructions that are
   executed, and thus increase by a factor of 4 the amount of time (for
   whatever the speed of the computer is).

   Thus, the constant 8 is irrelevant when asking this "doubling" question. 

   Note if we didn't simplify, we'd have

    Isort(2N)     8(2N)^2 + 12(2N) + 6       32N^2 + 24N + 6
   ----------- = ---------------------- = -------------------
    Isort(N)      8N^2 + 12N + 6              8N^2 + 12N + 6

   which doesn't simplify easiy; although, as N-&gt;infinty, this ratio gets
   closer and closer to 4 (and is very close even for relatively small-sized
   problems, because the coefficients are the same magnitude).

As with air-resistance and friction in physics, typically ignoring the
contribution of these negligible factors (for big, slow-moving objects) allows
us to quickly and easily solve an approximately correct problem.

Using big-O notation, we say that the complexity class of the code to find the
maximum is O(N). The big-O means "bounded by growth on the order of" N, which
means a linear growth (double the input size, double the time). For the sorting
code, its complexity class is O(N^2), which means "bounded by growth on the
order of N^2", which means a quadratic growth rate (double the input size,
quadruple the time).

----------
IMPORTANT: A Quick way to compute the complexity class of an algorithm

To analyze a C++ function's code and compute its complexity class, we
approximate the number of times the most frequently executed statement is
executed, dropping all the lower (more slowly growing) terms and dropping the
constant in front of the most frequently executed statement (the fastest
growing term). We will show how to do this much more rigorously in the next
lecture.

Here is how we apply this simple rule to the maximum code: it executes the if
statement N times, so it is O(N). The sorting code executes the if statement
N(N-1)/2 times (we will justify this number below), which is N^2/2 - N/2, so
dropping the lower term and the constant 1/2, yields a complexity class of
O(N^2).

In the next lecture we will generalize this method of calculating the complexity
class of C++ code.
----------

------------------------------------------------------------------------------

Comparing Algorithms by their complexity classes

Primarily from this definition we know that if two algorithms, a and b, both
solve some problem, and a is in a lower complexity class than b, then for all
BIG ENOUGH N, Ta(N) parameters/copying a pointer   | T(2N) = T(N)
O(LogN) | binary searching of a sorted array		| T(2N) = c + T(N)
O(N)	| linear searching an array    			| T(2N) = 2T(N)
O(NLogN)| Fast sorting					| T(2N) = cN + 2T(N)

  Fast algorithms come before here; NLogN grows a bit more slowly than linearly
  (because logarithms grow so slowly compared to N) and nowhere near as fast as
  O(N^2)

O(N^2) | Slow sorting; scanning N times array size N    | T(2N) = 4T(N)
O(N^3) | Slow matrix multiplication		        | T(2N) = 8T(N)
O(N^m) | Graph algorithms, for some fixed m: 4, 5, ...	| T(2N) = 2^mT(N)

  Tractable algorithms come before here; their work is polynomial in N.
  In the complexity class below, N is in an exponent.

O(2^N) | Finding boolean values that satisfy a formula  | T(2N)=2^N T(N)

For example, for an O(N^2) algorithm, doubling the size of the problem
quadruples the time required: T(2N) ~ c(2N)^2 = c4N^2 = 4cN^2 = 4T(N).

Note that in Computer Science, logarithms are mostly taken to base 2. (Remember
that algorithms and logarithms are very different terms). So when we write
logarithms, they are implicitly to base 2 (e.g., Log N = Log2 N). You should
memorize and be able to use the following facts to approximate logarithms
without a calculator.

Log 1000 ~ 10
  Actually, 2^10 = 1,024, 2^10 is approximatley 1,000 with  n0, f(n) 0
  (2) log n 0 (log is undefined if n=0
  (4) n log n 0 (by multiplying each side in (2) by n)

So, let's formally prove thta f(n) = 5n^2 + 3nlogn + 2n + 5 is O(n^2). That
means we must find an n0 and c such that

  5n^2 + 3nlogn + 2n + 5 n0

Note from (4) 3n log n  0,
          (3) 2n        0, and
          (1) 5         0

  and trivially 5n^2  0) with the result
5n^2 + 3nlogn + 2n + 5 0 (because
each term on the left is less than its corresponding term on the right for
n&gt;0). We can simplify all the terms on the right (they are all constants times
n^2) to be (5 + 3 + 2 + 5)n^2 or just 15n^2.

So, we have proven f(n) = 5n^2 + 3nlogn + 2n + 5 0,
therefore 5n^2 + 3nlogn + 2n + 5 is O(n^2) (with c = 15 and n0 = 0).


Likewise, we can see that for f(n) = 2n + 100log n, f(n) is O(n) because

 100log n 0) -multiply each side of (2) by 100, and
 2n + 100logn 0 - add 2n to each side.


Likewise, we can see that for f(n) = 2n^2 - 100n + 50, f(n) is O(n^2) because

  f(n) 0) = 2n^2+50  0)

By a similar "addition trick" we can can drop/ignore all negative terms that
are in a formula f when computing its complexity class.

 
Finally, we can see that for f(n) = 2n + 10, f(n) is O(n) because

  10 0)  -multiply each side of (1) by 100, so
  2n + 10 0

-------
There is more than one way to compute the c and n0 (which are not independent:
they depend on each other, in interesting ways). For example, there are many
other ways to show that for f(n) = 2n + 10, f(n) is O(N). Originally we chose
c = 12, and found f(n) 0. But, we can also choose c = 4, and use
that value to find for what n0, f(n) = 2n + 10 =5. So, we have c=4 and n0 = 4 as
another demonstration that f(n) is O(n).

If we choose c = 2.001, we can solve for when

  2n + 10 =10,000. So, we have c = 2.001 and
n0 = 9,999 as another demonstration that f(n) is O(n).

So, there are MULTPLE PROOFS (multiple cs and n0s) that meet the requirement
stated at the beginning. If can choose a large c, we can choose a small n0; if
we choose a smaller c, we will need to choose a larger n0.

Note that we must choose a c bigger than 2.0 (even a tad bigger: .001 as was
shown above, or even .0000001). The inequality

  2n + 10 = n		divide each side by -.5 (inequality changes =)

is meaningless: it isn't in the form, n &gt;= something. It concerns problem
sizes less than -20, and -20 is not a legal problem size!
-------

Finally, notice that if f(n) is O(n^2) then f(n) -by the definition- is also
O(n^3). Here is the proof. If f(n) is O(n^2) then we know

  for all n &gt; n0, f(n) =0 (and n is an integer and always &gt;= 0)

so for all n &gt; n0, f(n)  