 Survey of binary search data structures

General principle:
    handle insert, delete, successor, predecessor in logarithmic time
    "balanced" tree: each path has length O(log n)
    each operation modifies items on a single path
    various mechanisms for forcing tree to stay balanced
    various additional advantages/disadvantages beyond e.g. treaps
	faster for certain types of operation
	storage space
	...

treap:
	described last time
	used in LEDA algorithm library
	advantage:
		same properties as random-insert model for any seq of ops
		data structure state does not depend on history
		average access 2 ln n, relatively good
		conceptually simple 
	disadvantages:
		deepest path larger (by constant) than other balanced trees
		randomized

skip list:
	each new item flips coin until getting heads
		makes array of pointers #flips long
	pointer[i] -&gt; sorted singly linked list of items with &gt;= i flips
	search(x):
		i = max(#flips)
		pos = first item with i flips
		while i &gt;= 0:
			if x = 2a-1
		(so search step at each node more complicated)
	insert:
		find place to add
		if too many children, split and zipper upwards
	delete:
		remove child, rebalance w/sibling if necessary
	# nodes touched by update = O(log_a n)

	(2,3)-tree: O(log n) per update
	advantage: none?

	B-tree: node size = memory page, O(log_B n) per update
	advantage: adapts to memory hierarchy

red-black tree: [CLRS 13]
	binary search tree (items stored in both internal nodes and leaves)
	each node stores one bit: colored red or black

	root must be black
	any child of a red node must be black
	all paths root-&gt;null pointer have same # black nodes

	view as similar to 2-4 tree:
		group red nodes with their parents into single supernode
		=&gt; height grandparent and swap colors

	delete: similar with more cases (read the book)

	advantages:
		little extra memory (one bit per node)
		each insert does at most two rotations
			(useful for persistence, later)

AVL trees:
	first balanced bst: Adel'son, Velskii, Landis 1962.

	at each node, heights of subtrees differ by at most one
	each node stores two bits: which of left and right is deeper
	similar case analysis and rotation-based updates as red-black trees

	advantages:
		little extra memory
		more balanced than red-black:
			height log_phi n instead of 2 log_2 n

BB[alpha]-trees:
	at each node, #descendants(child)  amortized time O(log n)

	advantages:
		can get height (1+epsilon) log_2 n
			at cost of more frequent rebuilds, slower amort update
		can store large complicated data structures at each node,
			rebuild them infrequently, not worry about rotate
	disadvantages:
		time is amortized instead of worst case

scapegoat trees:
	no extra information per node (not even one bit), just tree structure
	otherwise similar to BB[alpha]-tree
	if insert places node too deep,
		computes balance of larger and larger subtrees at ancestors
		until finding non-alpha-balanced subtree
			then rebuilds that subtree to be balanced
		search time adds in geometric series to rebuild time

	time per search worst case O(log n)
	time per update amortized O(log n)

weighted binary trees (e.g. BB[alpha]-trees with total weight instead of #)
	time to insert/del/find item i: O(log W/w_i)
	advantages:
		can speed up times for frequently accessed items
	disadvantages:
		more complex
		
finger search trees (locality of reference)
	time to insert/del find item i: O(log |pos_i - pos_{i-1}|)
	advantages:
		locality of reference in input sequence
		(e.g. sequential order =&gt; linear time)
	disadvantages:
		more complex

Splay trees:
	next time
	advantages:
		most of above
		no extra space, weighted w/o change to data struc,
			finger searches (again w/o change)
	disadvantages:
		amortized
 